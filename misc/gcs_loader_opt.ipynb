{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "402ef665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nShapeNet Core GCS Loader for GCE VM Projects\\n============================================\\n\\nThis script downloads and extracts all 55 ShapeNet Core zip files from \\nGoogle Cloud Storage to your GCE VM project.\\n\\nUsage:\\n    1. Update the configuration variables below\\n    2. Run the script: python shapenet_loader.py\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ShapeNet Core GCS Loader for GCE VM Projects\n",
    "============================================\n",
    "\n",
    "This script downloads and extracts all 55 ShapeNet Core zip files from \n",
    "Google Cloud Storage to your GCE VM project.\n",
    "\n",
    "Usage:\n",
    "    1. Update the configuration variables below\n",
    "    2. Run the script: python shapenet_loader.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c32937c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from google.auth import default\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e82cc",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "CONFIGURATION - UPDATE THESE VALUES FOR YOUR PROJECT\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a4d2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"adlr2025\"           # Replace with your GCP project ID\n",
    "BUCKET_NAME = \"shapenet_bucket\"         # Replace with your GCS bucket name\n",
    "LOCAL_DATA_DIR = \"../src/data/shapenet_data\"       # Local directory for ShapeNet data\n",
    "EXTRACT_DIR = \"../src/data/shapenet_extracted\"     # Directory for extracted models\n",
    "MAX_WORKERS = 2                          # Number of parallel downloads/extractions (reduced for stability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c993b37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance tuning options\n",
    "EXTRACTION_BATCH_SIZE = 5                # How many zips to extract simultaneously\n",
    "PROGRESS_INTERVAL = 5000                 # Show progress every N files during extraction\n",
    "USE_SSD_OPTIMIZATIONS = True             # Enable optimizations for SSD storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62424329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category filtering - Set to None to extract all, or specify categories to extract\n",
    "# CATEGORIES_TO_EXTRACT = None             # Extract all categories\n",
    "CATEGORIES_TO_EXTRACT = [\"02946921\", \"02880940\", \"03085013\"]  # Only airplane, car, chair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27ae1a7d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ShapeNet Core typically has these categories (you can modify based on your bucket structure)\n",
    "SHAPENET_CATEGORIES = [\n",
    "    \"02691156\",  # airplane\n",
    "    \"02773838\",  # bag\n",
    "    \"02801938\",  # basket\n",
    "    \"02808440\",  # bathtub\n",
    "    \"02818832\",  # bed\n",
    "    \"02828884\",  # bench\n",
    "    \"02876657\",  # bottle\n",
    "    \"02880940\",  # bowl\n",
    "    \"02924116\",  # bus\n",
    "    \"02933112\",  # cabinet\n",
    "    \"02747177\",  # trash can\n",
    "    \"02942699\",  # camera\n",
    "    \"02954340\",  # cap\n",
    "    \"02958343\",  # car\n",
    "    \"03001627\",  # chair\n",
    "    \"03046257\",  # clock\n",
    "    \"03207941\",  # dishwasher\n",
    "    \"03211117\",  # display\n",
    "    \"03261776\",  # earphone\n",
    "    \"03325088\",  # faucet\n",
    "    \"03337140\",  # file cabinet\n",
    "    \"03467517\",  # guitar\n",
    "    \"03513137\",  # helmet\n",
    "    \"03593526\",  # jar\n",
    "    \"03624134\",  # knife\n",
    "    \"03636649\",  # lamp\n",
    "    \"03642806\",  # laptop\n",
    "    \"03691459\",  # loudspeaker\n",
    "    \"03710193\",  # mailbox\n",
    "    \"03759954\",  # microphone\n",
    "    \"03761084\",  # microwave\n",
    "    \"03790512\",  # motorbike\n",
    "    \"03797390\",  # mug\n",
    "    \"03928116\",  # piano\n",
    "    \"03938244\",  # pillow\n",
    "    \"03948459\",  # pistol\n",
    "    \"03991062\",  # pot\n",
    "    \"04004475\",  # printer\n",
    "    \"04074963\",  # remote control\n",
    "    \"04090263\",  # rifle\n",
    "    \"04099429\",  # rocket\n",
    "    \"04225987\",  # skateboard\n",
    "    \"04256520\",  # sofa\n",
    "    \"04330267\",  # stove\n",
    "    \"04379243\",  # table\n",
    "    \"04401088\",  # telephone\n",
    "    \"04460130\",  # tower\n",
    "    \"04468005\",  # train\n",
    "    \"04530566\",  # vessel\n",
    "    \"04554684\",  # washer\n",
    "    \"02992529\",  # cellphone\n",
    "    \"03085013\",  # keyboard\n",
    "    \"03366839\",  # folder\n",
    "    \"04401088\",  # phone\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977244a",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SETUP AND AUTHENTICATION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a5cc42e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def setup_environment():\n",
    "    \"\"\"Set up the local environment and authenticate with Google Cloud\"\"\"\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
    "    os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(\"ğŸš€ ShapeNet Core Loader Starting...\")\n",
    "    print(f\"Project ID: {PROJECT_ID}\")\n",
    "    print(f\"Bucket Name: {BUCKET_NAME}\")\n",
    "    print(f\"Download Directory: {LOCAL_DATA_DIR}\")\n",
    "    print(f\"Extract Directory: {EXTRACT_DIR}\")\n",
    "    print(f\"Max Workers: {MAX_WORKERS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Authenticate with Google Cloud\n",
    "    try:\n",
    "        credentials, project = default()\n",
    "        client = storage.Client(project=PROJECT_ID, credentials=credentials)\n",
    "        print(\"âœ“ Successfully authenticated using default credentials\")\n",
    "        print(f\"âœ“ Using project: {project}\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Authentication failed: {e}\")\n",
    "        print(\"ğŸ’¡ Make sure your GCE VM has the proper service account permissions\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d01bbc1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def connect_to_bucket(client, bucket_name):\n",
    "    \"\"\"Connect to the specified GCS bucket\"\"\"\n",
    "    try:\n",
    "        bucket = client.bucket(bucket_name)\n",
    "        \n",
    "        if bucket.exists():\n",
    "            print(f\"âœ“ Successfully connected to bucket: {bucket_name}\")\n",
    "            bucket.reload()\n",
    "            print(f\"âœ“ Bucket location: {bucket.location}\")\n",
    "            return bucket\n",
    "        else:\n",
    "            print(f\"âŒ Bucket {bucket_name} does not exist or is not accessible\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error connecting to bucket: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c6c61",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SHAPENET DISCOVERY FUNCTIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16f3ca71",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def discover_shapenet_zips(bucket, prefix=\"\"):\n",
    "    \"\"\"Discover all zip files in the bucket that contain ShapeNet data\"\"\"\n",
    "    print(f\"\\nğŸ” Discovering ShapeNet zip files...\")\n",
    "    \n",
    "    blobs = list(bucket.list_blobs(prefix=prefix))\n",
    "    zip_files = []\n",
    "    \n",
    "    for blob in blobs:\n",
    "        if blob.name.lower().endswith('.zip'):\n",
    "            size_mb = blob.size / (1024 * 1024) if blob.size else 0\n",
    "            zip_files.append({\n",
    "                'name': blob.name,\n",
    "                'size_mb': round(size_mb, 2),\n",
    "                'updated': blob.updated,\n",
    "                'blob': blob\n",
    "            })\n",
    "    \n",
    "    print(f\"ğŸ“¦ Found {len(zip_files)} zip files:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    total_size = 0\n",
    "    for zf in zip_files:\n",
    "        print(f\"ğŸ“„ {zf['name']} ({zf['size_mb']:.2f} MB)\")\n",
    "        total_size += zf['size_mb']\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Total size: {total_size:.2f} MB ({total_size/1024:.2f} GB)\")\n",
    "    \n",
    "    return zip_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b945d30",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def filter_zip_files_by_category(zip_files, categories_to_extract=None):\n",
    "    \"\"\"Filter zip files to only include specified categories\"\"\"\n",
    "    if categories_to_extract is None:\n",
    "        print(\"ğŸ“¦ Processing all categories\")\n",
    "        return zip_files\n",
    "    \n",
    "    print(f\"ğŸ¯ Filtering for categories: {categories_to_extract}\")\n",
    "    \n",
    "    filtered_files = []\n",
    "    for zip_info in zip_files:\n",
    "        # Check if any of the specified categories is in the filename\n",
    "        for category in categories_to_extract:\n",
    "            if category in zip_info['name']:\n",
    "                filtered_files.append(zip_info)\n",
    "                print(f\"  âœ“ Including: {zip_info['name']}\")\n",
    "                break\n",
    "    \n",
    "    excluded_count = len(zip_files) - len(filtered_files)\n",
    "    print(f\"ğŸ“Š Filtered result: {len(filtered_files)} files selected, {excluded_count} excluded\")\n",
    "    \n",
    "    return filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccaea969",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def categorize_zip_files(zip_files):\n",
    "    \"\"\"Try to categorize zip files by ShapeNet category\"\"\"\n",
    "    categorized = {}\n",
    "    uncategorized = []\n",
    "    \n",
    "    for zf in zip_files:\n",
    "        category_found = False\n",
    "        for category in SHAPENET_CATEGORIES:\n",
    "            if category in zf['name']:\n",
    "                if category not in categorized:\n",
    "                    categorized[category] = []\n",
    "                categorized[category].append(zf)\n",
    "                category_found = True\n",
    "                break\n",
    "        \n",
    "        if not category_found:\n",
    "            uncategorized.append(zf)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Categorization Summary:\")\n",
    "    print(f\"  â€¢ Categorized: {len(categorized)} categories\")\n",
    "    print(f\"  â€¢ Uncategorized: {len(uncategorized)} files\")\n",
    "    \n",
    "    return categorized, uncategorized\n",
    "    \"\"\"Try to categorize zip files by ShapeNet category\"\"\"\n",
    "    categorized = {}\n",
    "    uncategorized = []\n",
    "    \n",
    "    for zf in zip_files:\n",
    "        category_found = False\n",
    "        for category in SHAPENET_CATEGORIES:\n",
    "            if category in zf['name']:\n",
    "                if category not in categorized:\n",
    "                    categorized[category] = []\n",
    "                categorized[category].append(zf)\n",
    "                category_found = True\n",
    "                break\n",
    "        \n",
    "        if not category_found:\n",
    "            uncategorized.append(zf)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Categorization Summary:\")\n",
    "    print(f\"  â€¢ Categorized: {len(categorized)} categories\")\n",
    "    print(f\"  â€¢ Uncategorized: {len(uncategorized)} files\")\n",
    "    \n",
    "    return categorized, uncategorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef8455",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "DOWNLOAD FUNCTIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19c0cd70",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def download_zip_file(bucket, zip_info, local_dir, progress_callback=None):\n",
    "    \"\"\"Download a single zip file\"\"\"\n",
    "    blob_name = zip_info['name']\n",
    "    local_path = os.path.join(local_dir, os.path.basename(blob_name))\n",
    "    \n",
    "    try:\n",
    "        blob = bucket.blob(blob_name)\n",
    "        \n",
    "        # Check if file already exists\n",
    "        if os.path.exists(local_path):\n",
    "            local_size = os.path.getsize(local_path)\n",
    "            remote_size = blob.size\n",
    "            if local_size == remote_size:\n",
    "                print(f\"â­ï¸  Skipping {blob_name} (already exists)\")\n",
    "                return local_path, True\n",
    "        \n",
    "        print(f\"â¬‡ï¸  Downloading {blob_name} ({zip_info['size_mb']:.2f} MB)...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        blob.download_to_filename(local_path)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        speed = zip_info['size_mb'] / elapsed if elapsed > 0 else 0\n",
    "        \n",
    "        print(f\"âœ“ Downloaded {blob_name} in {elapsed:.1f}s ({speed:.1f} MB/s)\")\n",
    "        \n",
    "        if progress_callback:\n",
    "            progress_callback(blob_name, True)\n",
    "            \n",
    "        return local_path, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error downloading {blob_name}: {e}\")\n",
    "        if progress_callback:\n",
    "            progress_callback(blob_name, False)\n",
    "        return None, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c2b30af",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def download_all_zips(bucket, zip_files, local_dir, max_workers=MAX_WORKERS):\n",
    "    \"\"\"Download all zip files using parallel workers\"\"\"\n",
    "    print(f\"\\nâ¬‡ï¸  Starting download of {len(zip_files)} files using {max_workers} workers...\")\n",
    "    \n",
    "    downloaded_files = []\n",
    "    failed_downloads = []\n",
    "    \n",
    "    # Progress tracking\n",
    "    completed = threading.Event()\n",
    "    progress_lock = threading.Lock()\n",
    "    progress_data = {'completed': 0, 'total': len(zip_files), 'failed': 0}\n",
    "    \n",
    "    def progress_callback(filename, success):\n",
    "        with progress_lock:\n",
    "            progress_data['completed'] += 1\n",
    "            if not success:\n",
    "                progress_data['failed'] += 1\n",
    "            print(f\"ğŸ“Š Progress: {progress_data['completed']}/{progress_data['total']} \"\n",
    "                  f\"(Failed: {progress_data['failed']})\")\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel downloads\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all download tasks\n",
    "        future_to_zip = {\n",
    "            executor.submit(download_zip_file, bucket, zf, local_dir, progress_callback): zf \n",
    "            for zf in zip_files\n",
    "        }\n",
    "        \n",
    "        # Collect results\n",
    "        for future in as_completed(future_to_zip):\n",
    "            zip_info = future_to_zip[future]\n",
    "            try:\n",
    "                local_path, success = future.result()\n",
    "                if success and local_path:\n",
    "                    downloaded_files.append(local_path)\n",
    "                else:\n",
    "                    failed_downloads.append(zip_info['name'])\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Download failed for {zip_info['name']}: {e}\")\n",
    "                failed_downloads.append(zip_info['name'])\n",
    "    \n",
    "    print(f\"\\nğŸ“¦ Download Summary:\")\n",
    "    print(f\"  âœ“ Successfully downloaded: {len(downloaded_files)}\")\n",
    "    print(f\"  âŒ Failed downloads: {len(failed_downloads)}\")\n",
    "    \n",
    "    if failed_downloads:\n",
    "        print(f\"  Failed files: {failed_downloads}\")\n",
    "    \n",
    "    return downloaded_files, failed_downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e64bb",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "EXTRACTION FUNCTIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfb73f3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_zip_file(zip_path, extract_dir, preserve_structure=True, progress_interval=5000):\n",
    "    \"\"\"Extract a single zip file with optimized performance\"\"\"\n",
    "    try:\n",
    "        zip_name = os.path.basename(zip_path)\n",
    "        print(f\"ğŸ“‚ Extracting {zip_name}...\")\n",
    "        \n",
    "        # Create extraction subdirectory if preserving structure\n",
    "        if preserve_structure:\n",
    "            extract_subdir = os.path.join(extract_dir, os.path.splitext(zip_name)[0])\n",
    "            os.makedirs(extract_subdir, exist_ok=True)\n",
    "            final_extract_dir = extract_subdir\n",
    "        else:\n",
    "            final_extract_dir = extract_dir\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            # Get info about the zip\n",
    "            file_list = zip_ref.infolist()\n",
    "            file_count = len(file_list)\n",
    "            \n",
    "            print(f\"  ğŸ“„ {file_count} files to extract...\")\n",
    "            \n",
    "            # Extract files with progress updates for large archives\n",
    "            if file_count > 1000:\n",
    "                extracted = 0\n",
    "                last_update = 0\n",
    "                \n",
    "                for file_info in file_list:\n",
    "                    # Skip directories\n",
    "                    if file_info.is_dir():\n",
    "                        continue\n",
    "                        \n",
    "                    # Extract individual file\n",
    "                    zip_ref.extract(file_info, final_extract_dir)\n",
    "                    extracted += 1\n",
    "                    \n",
    "                    # Progress update every N files\n",
    "                    if extracted - last_update >= progress_interval:\n",
    "                        percent = (extracted / file_count) * 100\n",
    "                        elapsed_so_far = time.time() - start_time\n",
    "                        rate = extracted / elapsed_so_far if elapsed_so_far > 0 else 0\n",
    "                        eta = (file_count - extracted) / rate if rate > 0 else 0\n",
    "                        \n",
    "                        print(f\"    Progress: {extracted}/{file_count} ({percent:.1f}%) \"\n",
    "                              f\"[{rate:.0f} files/s, ETA: {eta/60:.1f}m]\")\n",
    "                        last_update = extracted\n",
    "            else:\n",
    "                # For smaller archives, extract normally\n",
    "                zip_ref.extractall(final_extract_dir)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        rate = file_count / elapsed if elapsed > 0 else 0\n",
    "        print(f\"âœ“ Extracted {zip_name} ({file_count} files) in {elapsed:.1f}s ({rate:.0f} files/s)\")\n",
    "        \n",
    "        return final_extract_dir, file_count, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error extracting {zip_path}: {e}\")\n",
    "        return None, 0, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65951c35",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_all_zips(zip_files, extract_dir, max_workers=EXTRACTION_BATCH_SIZE, preserve_structure=True):\n",
    "    \"\"\"Extract all zip files using parallel workers with performance optimizations\"\"\"\n",
    "    print(f\"\\nğŸ“‚ Starting extraction of {len(zip_files)} files...\")\n",
    "    print(f\"   Using {max_workers} parallel extractions\")\n",
    "    print(f\"   Progress updates every {PROGRESS_INTERVAL} files\")\n",
    "    \n",
    "    extracted_dirs = []\n",
    "    failed_extractions = []\n",
    "    total_files = 0\n",
    "    \n",
    "    # Sort zip files by size (smallest first for better load balancing)\n",
    "    zip_files_sorted = sorted(zip_files, key=lambda x: os.path.getsize(x) if os.path.exists(x) else 0)\n",
    "    \n",
    "    # Progress tracking\n",
    "    progress_lock = threading.Lock()\n",
    "    progress_data = {'completed': 0, 'total': len(zip_files), 'failed': 0}\n",
    "    \n",
    "    def extract_with_progress(zip_path):\n",
    "        result_dir, file_count, success = extract_zip_file(\n",
    "            zip_path, extract_dir, preserve_structure, PROGRESS_INTERVAL\n",
    "        )\n",
    "        \n",
    "        with progress_lock:\n",
    "            progress_data['completed'] += 1\n",
    "            if not success:\n",
    "                progress_data['failed'] += 1\n",
    "            \n",
    "            remaining = progress_data['total'] - progress_data['completed']\n",
    "            print(f\"ğŸ“Š Overall Progress: {progress_data['completed']}/{progress_data['total']} \"\n",
    "                  f\"({remaining} remaining, {progress_data['failed']} failed)\")\n",
    "        \n",
    "        return result_dir, file_count, success, zip_path\n",
    "    \n",
    "    # Use ThreadPoolExecutor with reduced workers for extraction\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all extraction tasks\n",
    "        futures = [executor.submit(extract_with_progress, zip_path) for zip_path in zip_files_sorted]\n",
    "        \n",
    "        # Collect results\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result_dir, file_count, success, zip_path = future.result()\n",
    "                if success:\n",
    "                    extracted_dirs.append(result_dir)\n",
    "                    total_files += file_count\n",
    "                else:\n",
    "                    failed_extractions.append(zip_path)\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Extraction failed: {e}\")\n",
    "                failed_extractions.append(\"unknown\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ Extraction Summary:\")\n",
    "    print(f\"  âœ“ Successfully extracted: {len(extracted_dirs)} archives\")\n",
    "    print(f\"  ğŸ“„ Total files extracted: {total_files:,}\")\n",
    "    print(f\"  âŒ Failed extractions: {len(failed_extractions)}\")\n",
    "    \n",
    "    return extracted_dirs, failed_extractions, total_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181539ea",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "ANALYSIS FUNCTIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a143f19f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_shapenet_structure(extract_dir):\n",
    "    \"\"\"Analyze the structure of extracted ShapeNet data\"\"\"\n",
    "    print(f\"\\nğŸ” Analyzing ShapeNet data structure in {extract_dir}...\")\n",
    "    \n",
    "    structure_info = {}\n",
    "    total_models = 0\n",
    "    \n",
    "    extract_path = Path(extract_dir)\n",
    "    \n",
    "    for category_dir in extract_path.iterdir():\n",
    "        if category_dir.is_dir():\n",
    "            category_name = category_dir.name\n",
    "            \n",
    "            # Count models in this category\n",
    "            model_count = 0\n",
    "            model_dirs = []\n",
    "            \n",
    "            for item in category_dir.rglob('*'):\n",
    "                if item.is_dir() and len(item.name) == 32:  # ShapeNet model IDs are 32 chars\n",
    "                    model_count += 1\n",
    "                    model_dirs.append(item)\n",
    "            \n",
    "            structure_info[category_name] = {\n",
    "                'model_count': model_count,\n",
    "                'path': str(category_dir),\n",
    "                'model_dirs': model_dirs[:5]  # Store first 5 for sampling\n",
    "            }\n",
    "            \n",
    "            total_models += model_count\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ShapeNet Structure Analysis:\")\n",
    "    print(f\"  ğŸ“‚ Categories found: {len(structure_info)}\")\n",
    "    print(f\"  ğŸ¯ Total models: {total_models}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for category, info in structure_info.items():\n",
    "        print(f\"  {category}: {info['model_count']} models\")\n",
    "    \n",
    "    return structure_info, total_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a960c6a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def sample_model_files(structure_info, sample_size=3):\n",
    "    \"\"\"Sample some model files to understand the data format\"\"\"\n",
    "    print(f\"\\nğŸ”¬ Sampling model files (up to {sample_size} per category)...\")\n",
    "    \n",
    "    file_types = {}\n",
    "    \n",
    "    for category, info in structure_info.items():\n",
    "        if info['model_dirs']:\n",
    "            print(f\"\\nğŸ“‚ Category: {category}\")\n",
    "            \n",
    "            for i, model_dir in enumerate(info['model_dirs'][:sample_size]):\n",
    "                print(f\"  Model {i+1}: {model_dir.name}\")\n",
    "                \n",
    "                # List files in this model directory\n",
    "                for file_path in model_dir.iterdir():\n",
    "                    if file_path.is_file():\n",
    "                        ext = file_path.suffix.lower()\n",
    "                        if ext not in file_types:\n",
    "                            file_types[ext] = 0\n",
    "                        file_types[ext] += 1\n",
    "                        print(f\"    ğŸ“„ {file_path.name}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ File type summary:\")\n",
    "    for ext, count in sorted(file_types.items()):\n",
    "        print(f\"  {ext}: {count} files\")\n",
    "    \n",
    "    return file_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd7940",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "UTILITY FUNCTIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68b54b36",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def cleanup_zip_files(zip_files, keep_zips=False):\n",
    "    \"\"\"Clean up downloaded zip files after extraction\"\"\"\n",
    "    if keep_zips:\n",
    "        print(\"\\nğŸ’¾ Keeping zip files as requested\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nğŸ§¹ Cleaning up {len(zip_files)} zip files...\")\n",
    "    \n",
    "    removed_count = 0\n",
    "    for zip_path in zip_files:\n",
    "        try:\n",
    "            if os.path.exists(zip_path):\n",
    "                os.remove(zip_path)\n",
    "                removed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error removing {zip_path}: {e}\")\n",
    "    \n",
    "    print(f\"âœ“ Removed {removed_count} zip files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bce975f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_shapenet_manifest(structure_info, total_models, extract_dir):\n",
    "    \"\"\"Save a manifest of the ShapeNet data\"\"\"\n",
    "    manifest = {\n",
    "        \"total_categories\": len(structure_info),\n",
    "        \"total_models\": total_models,\n",
    "        \"extract_directory\": extract_dir,\n",
    "        \"categories\": {}\n",
    "    }\n",
    "    \n",
    "    for category, info in structure_info.items():\n",
    "        manifest[\"categories\"][category] = {\n",
    "            \"model_count\": info[\"model_count\"],\n",
    "            \"path\": info[\"path\"]\n",
    "        }\n",
    "    \n",
    "    manifest_path = os.path.join(extract_dir, \"shapenet_manifest.json\")\n",
    "    with open(manifest_path, 'w') as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    \n",
    "    print(f\"ğŸ“„ Manifest saved to: {manifest_path}\")\n",
    "    return manifest_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e4a63a",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "MAIN EXECUTION FUNCTION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8744285",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the ShapeNet loading process\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ¯ SHAPENET CORE LOADER\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Setup and authentication\n",
    "    client = setup_environment()\n",
    "    if not client:\n",
    "        return\n",
    "    \n",
    "    # Step 2: Connect to bucket\n",
    "    bucket = connect_to_bucket(client, BUCKET_NAME)\n",
    "    if not bucket:\n",
    "        return\n",
    "    \n",
    "    # Step 3: Discover ShapeNet zip files\n",
    "    zip_files = discover_shapenet_zips(bucket)\n",
    "    if not zip_files:\n",
    "        print(\"âŒ No zip files found in the bucket!\")\n",
    "        return\n",
    "    \n",
    "    # Step 4: Show categorization\n",
    "    categorized, uncategorized = categorize_zip_files(zip_files)\n",
    "    \n",
    "    # Step 4.5: Filter categories if specified\n",
    "    if CATEGORIES_TO_EXTRACT is not None:\n",
    "        zip_files = filter_zip_files_by_category(zip_files, CATEGORIES_TO_EXTRACT)\n",
    "        if not zip_files:\n",
    "            print(\"âŒ No files match the specified categories!\")\n",
    "            return\n",
    "    \n",
    "    # Step 5: Confirm download\n",
    "    total_size_gb = sum(zf['size_mb'] for zf in zip_files) / 1024\n",
    "    print(f\"\\nâ“ Ready to download {len(zip_files)} files ({total_size_gb:.2f} GB)?\")\n",
    "    print(f\"   This will use approximately {total_size_gb * 2:.1f} GB of disk space (zip + extracted)\")\n",
    "    \n",
    "    confirm = input(\"Continue? (y/n): \").strip().lower()\n",
    "    if confirm != 'y':\n",
    "        print(\"âŒ Download cancelled\")\n",
    "        return\n",
    "    \n",
    "    # Step 6: Download all zip files\n",
    "    downloaded_files, failed_downloads = download_all_zips(bucket, zip_files, LOCAL_DATA_DIR)\n",
    "    \n",
    "    if not downloaded_files:\n",
    "        print(\"âŒ No files were downloaded successfully!\")\n",
    "        return\n",
    "    \n",
    "    # Step 7: Extract all zip files\n",
    "    print(f\"\\nğŸ”„ Starting extraction phase...\")\n",
    "    keep_zips = input(\"Keep zip files after extraction? (y/n): \").strip().lower() == 'y'\n",
    "    \n",
    "    extracted_dirs, failed_extractions, total_files = extract_all_zips(\n",
    "        downloaded_files, EXTRACT_DIR, preserve_structure=True\n",
    "    )\n",
    "    \n",
    "    # Step 8: Analyze structure\n",
    "    structure_info, total_models = analyze_shapenet_structure(EXTRACT_DIR)\n",
    "    \n",
    "    # Step 9: Sample files\n",
    "    if structure_info:\n",
    "        sample_files = input(\"Sample model files to understand structure? (y/n): \").strip().lower() == 'y'\n",
    "        if sample_files:\n",
    "            file_types = sample_model_files(structure_info)\n",
    "    \n",
    "    # Step 10: Save manifest\n",
    "    manifest_path = save_shapenet_manifest(structure_info, total_models, EXTRACT_DIR)\n",
    "    \n",
    "    # Step 11: Cleanup\n",
    "    if not keep_zips:\n",
    "        cleanup_zip_files(downloaded_files, keep_zips=False)\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nğŸ‰ SHAPENET CORE LOADING COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ğŸ“Š Summary:\")\n",
    "    print(f\"  â€¢ Downloaded: {len(downloaded_files)} zip files\")\n",
    "    print(f\"  â€¢ Extracted: {len(extracted_dirs)} archives\")\n",
    "    print(f\"  â€¢ Total files: {total_files}\")\n",
    "    print(f\"  â€¢ Categories: {len(structure_info)}\")\n",
    "    print(f\"  â€¢ Total models: {total_models}\")\n",
    "    print(f\"  â€¢ Data location: {EXTRACT_DIR}\")\n",
    "    print(f\"  â€¢ Manifest: {manifest_path}\")\n",
    "    \n",
    "    if failed_downloads:\n",
    "        print(f\"  âš ï¸  Failed downloads: {len(failed_downloads)}\")\n",
    "    if failed_extractions:\n",
    "        print(f\"  âš ï¸  Failed extractions: {len(failed_extractions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55d6b647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ Before running, make sure to update:\n",
      "   â€¢ PROJECT_ID: adlr2025\n",
      "   â€¢ BUCKET_NAME: shapenet_bucket\n",
      "\n",
      "â–¶ï¸  Starting ShapeNet Core loader...\n",
      "============================================================\n",
      "ğŸ¯ SHAPENET CORE LOADER\n",
      "============================================================\n",
      "ğŸš€ ShapeNet Core Loader Starting...\n",
      "Project ID: adlr2025\n",
      "Bucket Name: shapenet_bucket\n",
      "Download Directory: ../src/data/shapenet_data\n",
      "Extract Directory: ../src/data/shapenet_extracted\n",
      "Max Workers: 2\n",
      "------------------------------------------------------------\n",
      "âœ“ Successfully authenticated using default credentials\n",
      "âœ“ Using project: adlr2025\n",
      "âœ“ Successfully connected to bucket: shapenet_bucket\n",
      "âœ“ Bucket location: EUROPE-WEST3\n",
      "\n",
      "ğŸ” Discovering ShapeNet zip files...\n",
      "ğŸ“¦ Found 55 zip files:\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ“„ 02691156.zip (3203.42 MB)\n",
      "ğŸ“„ 02747177.zip (61.14 MB)\n",
      "ğŸ“„ 02773838.zip (27.64 MB)\n",
      "ğŸ“„ 02801938.zip (30.65 MB)\n",
      "ğŸ“„ 02808440.zip (193.99 MB)\n",
      "ğŸ“„ 02818832.zip (69.93 MB)\n",
      "ğŸ“„ 02828884.zip (359.33 MB)\n",
      "ğŸ“„ 02843684.zip (4.21 MB)\n",
      "ğŸ“„ 02871439.zip (74.00 MB)\n",
      "ğŸ“„ 02876657.zip (85.31 MB)\n",
      "ğŸ“„ 02880940.zip (20.44 MB)\n",
      "ğŸ“„ 02924116.zip (692.97 MB)\n",
      "ğŸ“„ 02933112.zip (412.70 MB)\n",
      "ğŸ“„ 02942699.zip (32.26 MB)\n",
      "ğŸ“„ 02946921.zip (19.05 MB)\n",
      "ğŸ“„ 02954340.zip (42.52 MB)\n",
      "ğŸ“„ 02958343.zip (5422.08 MB)\n",
      "ğŸ“„ 02992529.zip (227.39 MB)\n",
      "ğŸ“„ 03001627.zip (1874.23 MB)\n",
      "ğŸ“„ 03046257.zip (177.57 MB)\n",
      "ğŸ“„ 03085013.zip (18.12 MB)\n",
      "ğŸ“„ 03207941.zip (10.36 MB)\n",
      "ğŸ“„ 03211117.zip (272.26 MB)\n",
      "ğŸ“„ 03261776.zip (22.32 MB)\n",
      "ğŸ“„ 03325088.zip (149.38 MB)\n",
      "ğŸ“„ 03337140.zip (73.16 MB)\n",
      "ğŸ“„ 03467517.zip (553.55 MB)\n",
      "ğŸ“„ 03513137.zip (36.93 MB)\n",
      "ğŸ“„ 03593526.zip (162.90 MB)\n",
      "ğŸ“„ 03624134.zip (61.49 MB)\n",
      "ğŸ“„ 03636649.zip (710.79 MB)\n",
      "ğŸ“„ 03642806.zip (218.79 MB)\n",
      "ğŸ“„ 03691459.zip (504.89 MB)\n",
      "ğŸ“„ 03710193.zip (10.84 MB)\n",
      "ğŸ“„ 03759954.zip (13.94 MB)\n",
      "ğŸ“„ 03761084.zip (33.95 MB)\n",
      "ğŸ“„ 03790512.zip (545.12 MB)\n",
      "ğŸ“„ 03797390.zip (23.50 MB)\n",
      "ğŸ“„ 03928116.zip (84.00 MB)\n",
      "ğŸ“„ 03938244.zip (27.04 MB)\n",
      "ğŸ“„ 03948459.zip (81.06 MB)\n",
      "ğŸ“„ 03991062.zip (298.55 MB)\n",
      "ğŸ“„ 04004475.zip (31.85 MB)\n",
      "ğŸ“„ 04074963.zip (10.36 MB)\n",
      "ğŸ“„ 04090263.zip (884.97 MB)\n",
      "ğŸ“„ 04099429.zip (21.16 MB)\n",
      "ğŸ“„ 04225987.zip (79.85 MB)\n",
      "ğŸ“„ 04256520.zip (1232.10 MB)\n",
      "ğŸ“„ 04330267.zip (79.45 MB)\n",
      "ğŸ“„ 04379243.zip (1589.76 MB)\n",
      "ğŸ“„ 04401088.zip (295.08 MB)\n",
      "ğŸ“„ 04460130.zip (60.40 MB)\n",
      "ğŸ“„ 04468005.zip (353.43 MB)\n",
      "ğŸ“„ 04530566.zip (1251.94 MB)\n",
      "ğŸ“„ 04554684.zip (25.41 MB)\n",
      "\n",
      "ğŸ“Š Total size: 22859.53 MB (22.32 GB)\n",
      "\n",
      "ğŸ“‹ Categorization Summary:\n",
      "  â€¢ Categorized: 52 categories\n",
      "  â€¢ Uncategorized: 3 files\n",
      "ğŸ¯ Filtering for categories: ['02946921', '02880940', '03085013']\n",
      "  âœ“ Including: 02880940.zip\n",
      "  âœ“ Including: 02946921.zip\n",
      "  âœ“ Including: 03085013.zip\n",
      "ğŸ“Š Filtered result: 3 files selected, 52 excluded\n",
      "\n",
      "â“ Ready to download 3 files (0.06 GB)?\n",
      "   This will use approximately 0.1 GB of disk space (zip + extracted)\n",
      "\n",
      "â¬‡ï¸  Starting download of 3 files using 2 workers...\n",
      "â¬‡ï¸  Downloading 02880940.zip (20.44 MB)...\n",
      "â¬‡ï¸  Downloading 02946921.zip (19.05 MB)...\n",
      "âœ“ Downloaded 02880940.zip in 0.5s (44.9 MB/s)\n",
      "ğŸ“Š Progress: 1/3 (Failed: 0)\n",
      "â¬‡ï¸  Downloading 03085013.zip (18.12 MB)...\n",
      "âœ“ Downloaded 02946921.zip in 0.5s (39.4 MB/s)\n",
      "ğŸ“Š Progress: 2/3 (Failed: 0)\n",
      "âœ“ Downloaded 03085013.zip in 0.3s (67.5 MB/s)\n",
      "ğŸ“Š Progress: 3/3 (Failed: 0)\n",
      "\n",
      "ğŸ“¦ Download Summary:\n",
      "  âœ“ Successfully downloaded: 3\n",
      "  âŒ Failed downloads: 0\n",
      "\n",
      "ğŸ”„ Starting extraction phase...\n",
      "\n",
      "ğŸ“‚ Starting extraction of 3 files...\n",
      "   Using 5 parallel extractions\n",
      "   Progress updates every 5000 files\n",
      "ğŸ“‚ Extracting 03085013.zip...\n",
      "ğŸ“‚ Extracting 02946921.zip...\n",
      "ğŸ“‚ Extracting 02880940.zip...\n",
      "  ğŸ“„ 525 files to extract...\n",
      "  ğŸ“„ 1814 files to extract...\n",
      "  ğŸ“„ 944 files to extract...\n",
      "âœ“ Extracted 02946921.zip (944 files) in 1.1s (892 files/s)\n",
      "ğŸ“Š Overall Progress: 1/3 (2 remaining, 0 failed)\n",
      "âœ“ Extracted 03085013.zip (525 files) in 1.2s (431 files/s)\n",
      "ğŸ“Š Overall Progress: 2/3 (1 remaining, 0 failed)\n",
      "âœ“ Extracted 02880940.zip (1814 files) in 1.5s (1235 files/s)\n",
      "ğŸ“Š Overall Progress: 3/3 (0 remaining, 0 failed)\n",
      "\n",
      "ğŸ“ Extraction Summary:\n",
      "  âœ“ Successfully extracted: 3 archives\n",
      "  ğŸ“„ Total files extracted: 3,283\n",
      "  âŒ Failed extractions: 0\n",
      "\n",
      "ğŸ” Analyzing ShapeNet data structure in ../src/data/shapenet_extracted...\n",
      "\n",
      "ğŸ“Š ShapeNet Structure Analysis:\n",
      "  ğŸ“‚ Categories found: 6\n",
      "  ğŸ¯ Total models: 12976\n",
      "------------------------------------------------------------\n",
      "  02958343: 3093 models\n",
      "  02880940: 167 models\n",
      "  03001627: 6005 models\n",
      "  02691156: 3567 models\n",
      "  02946921: 88 models\n",
      "  03085013: 56 models\n",
      "ğŸ“„ Manifest saved to: ../src/data/shapenet_extracted/shapenet_manifest.json\n",
      "\n",
      "ğŸ§¹ Cleaning up 3 zip files...\n",
      "âœ“ Removed 3 zip files\n",
      "\n",
      "ğŸ‰ SHAPENET CORE LOADING COMPLETE!\n",
      "============================================================\n",
      "ğŸ“Š Summary:\n",
      "  â€¢ Downloaded: 3 zip files\n",
      "  â€¢ Extracted: 3 archives\n",
      "  â€¢ Total files: 3283\n",
      "  â€¢ Categories: 6\n",
      "  â€¢ Total models: 12976\n",
      "  â€¢ Data location: ../src/data/shapenet_extracted\n",
      "  â€¢ Manifest: ../src/data/shapenet_extracted/shapenet_manifest.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸ’¡ Before running, make sure to update:\")\n",
    "    print(f\"   â€¢ PROJECT_ID: {PROJECT_ID}\")\n",
    "    print(f\"   â€¢ BUCKET_NAME: {BUCKET_NAME}\")\n",
    "    print(\"\\nâ–¶ï¸  Starting ShapeNet Core loader...\")\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
