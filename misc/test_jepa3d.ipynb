{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb7c82f-b71b-4e25-b2b5-d2b7267c9597",
   "metadata": {},
   "source": [
    "# ðŸ§ª Test JEPA3D Encoder\n",
    "\n",
    "This notebook shows how to import and run your `JEPA3DEncoderWrapper` wrapper from  \n",
    "`src/models/encoders/jepa3d_backbone.py`.\n",
    "\n",
    "The function `create_featurized_scene_dict` creates a suitable randomized input for the pretrained JEPA3D Encoder.\n",
    "\n",
    "Since the current VM is not compatible for using Flash Attention, all default arguments enable_flash in the `point_transfomer.py` file are assumed to be False (instead of True, like before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1f9dbf-59d7-495f-96e1-35a090ffe269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Both imports resolve!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/borispetrovic/.local/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(str(Path().resolve().parent / 'src'))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from models.encoders.jepa3d_wrapper import JEPA3DEncoderWrapper\n",
    "from ext.jepa3d.models.encoder_3djepa import Encoder3DJEPA\n",
    "\n",
    "print(\"âœ… Both imports resolve!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dbd9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_featurized_scene_dict(num_points=30000, device='cpu', model=None):\n",
    "    \"\"\"\n",
    "    Create a featurized scene dictionary that matches the expected input format\n",
    "    for the 3D-JEPA encoder based on the forward method.\n",
    "    \n",
    "    Args:\n",
    "        num_points: Number of points in the scene\n",
    "        device: Device to create tensors on\n",
    "        model: The model instance to check expected dimensions\n",
    "    \n",
    "    Returns:\n",
    "        dict: featurized_scene_dict with all required keys\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create XYZ coordinates - simulate a room-like environment\n",
    "    xyz = torch.zeros(num_points, 3, device=device)\n",
    "    \n",
    "    # Generate points that look like a room with furniture\n",
    "    # Floor points (z near 0)\n",
    "    floor_mask = torch.rand(num_points, device=device) < 0.3\n",
    "    xyz[floor_mask, :2] = torch.rand((floor_mask.sum(), 2), device=device) * 4 - 2  # -2 to 2 meters\n",
    "    xyz[floor_mask, 2] = torch.rand(floor_mask.sum(), device=device) * 0.1  # 0 to 0.1 meters\n",
    "    \n",
    "    # Ceiling points (z near 3)\n",
    "    ceiling_mask = (~floor_mask) & (torch.rand(num_points, device=device) < 0.2)\n",
    "    xyz[ceiling_mask, :2] = torch.rand((ceiling_mask.sum(), 2), device=device) * 4 - 2\n",
    "    xyz[ceiling_mask, 2] = 2.8 + torch.rand(ceiling_mask.sum(), device=device) * 0.2  # 2.8 to 3.0 meters\n",
    "    \n",
    "    # Wall and furniture points (middle z values)\n",
    "    remaining_mask = ~floor_mask & ~ceiling_mask\n",
    "    xyz[remaining_mask, :2] = torch.rand((remaining_mask.sum(), 2), device=device) * 4 - 2\n",
    "    xyz[remaining_mask, 2] = torch.rand(remaining_mask.sum(), device=device) * 2.8  # 0 to 2.8 meters\n",
    "\n",
    "    print(xyz.shape)\n",
    "    \n",
    "    # Create RGB colors (0-1 range - model will multiply by 255)\n",
    "    rgb = torch.rand(num_points, 3, device=device)\n",
    "    \n",
    "    # Floor tends to be darker/brownish\n",
    "    rgb[floor_mask] = torch.tensor([0.4, 0.3, 0.2], device=device) + torch.rand((floor_mask.sum(), 3), device=device) * 0.3\n",
    "    \n",
    "    # Ceiling tends to be white/light\n",
    "    rgb[ceiling_mask] = torch.tensor([0.8, 0.8, 0.8], device=device) + torch.rand((ceiling_mask.sum(), 3), device=device) * 0.2\n",
    "    \n",
    "    # Clamp RGB to [0, 1]\n",
    "    rgb = torch.clamp(rgb, 0, 1)\n",
    "    \n",
    "    # Determine feature dimensions based on model if available\n",
    "    if model is not None and hasattr(model, 'input_feat_dim'):\n",
    "        total_feat_dim = model.input_feat_dim\n",
    "        print(f\"Model expects {total_feat_dim} total feature dimensions\")\n",
    "        \n",
    "        # Split roughly evenly between CLIP and DINO\n",
    "        clip_feat_dim = total_feat_dim // 2\n",
    "        dino_feat_dim = total_feat_dim - clip_feat_dim\n",
    "        print(f\"Using CLIP: {clip_feat_dim}, DINO: {dino_feat_dim}\")\n",
    "    else:\n",
    "        # The error showed we need 896 total dimensions\n",
    "        # Current attempt: 512 + 384 = 896\n",
    "        total_feat_dim = 896\n",
    "        clip_feat_dim = 512  \n",
    "        dino_feat_dim = 384  \n",
    "        print(f\"Using default dimensions - CLIP: {clip_feat_dim}, DINO: {dino_feat_dim}, Total: {total_feat_dim}\")\n",
    "    \n",
    "    # Create CLIP and DINO features with correct dimensions\n",
    "    features_clip = torch.randn(num_points, clip_feat_dim, device=device) * 0.1\n",
    "    features_dino = torch.randn(num_points, dino_feat_dim, device=device) * 0.1\n",
    "    \n",
    "    # Create the featurized scene dictionary\n",
    "    featurized_scene_dict = {\n",
    "        \"features_clip\": features_clip,      # Shape: (num_points, clip_feat_dim)\n",
    "        \"features_dino\": features_dino,      # Shape: (num_points, dino_feat_dim)\n",
    "        \"rgb\": rgb,                          # Shape: (num_points, 3) in [0,1] range\n",
    "        \"points\": xyz,                       # Shape: (num_points, 3)\n",
    "    }\n",
    "    \n",
    "    return featurized_scene_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4692504b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 3])\n",
      "Model expects 1536 total feature dimensions\n",
      "Using CLIP: 768, DINO: 768\n",
      "zero_token device: cuda:0\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "GPU: Tesla T4\n",
      "CUDA Capability: (7, 5)\n",
      "Supports FlashAttention: False\n",
      "[DEBUG] features.shape = torch.Size([1, 1024, 1536])\n",
      "[DEBUG] zero_token.shape = torch.Size([1536])\n"
     ]
    }
   ],
   "source": [
    "# import the pretrained model\n",
    "model_3djepa = Encoder3DJEPA.from_pretrained(\"facebook/3d-jepa\")\n",
    "\n",
    "featurized_scene_dict = create_featurized_scene_dict(\n",
    "    num_points=1024, \n",
    "    model=model_3djepa,\n",
    "    device=torch.device('cuda')\n",
    ")\n",
    "\n",
    "# Load and move model to CUDA\n",
    "model_3djepa = Encoder3DJEPA.from_pretrained(\"facebook/3d-jepa\")\n",
    "model_3djepa = model_3djepa.cuda()\n",
    "\n",
    "# Explicitly move zero_token to CUDA\n",
    "if hasattr(model_3djepa, 'zero_token'):\n",
    "    model_3djepa.zero_token = model_3djepa.zero_token.cuda()\n",
    "\n",
    "# Check device\n",
    "print(f\"zero_token device: {model_3djepa.zero_token.device}\")\n",
    "# Check if you have CUDA\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "print(f\"CUDA Capability: {torch.cuda.get_device_capability()}\")\n",
    "\n",
    "# Ampere (supports FlashAttention): capability >= (8, 0)\n",
    "# Since our current VM doesn't support, flash attention was disabled in point_transformer_v3.py\n",
    "capability = torch.cuda.get_device_capability()\n",
    "is_ampere_or_newer = capability[0] >= 8\n",
    "print(f\"Supports FlashAttention: {is_ampere_or_newer}\")\n",
    "\n",
    "\n",
    "output = model_3djepa(featurized_scene_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a8fdfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_hub_mixin_config', 'voxel_size', 'input_feat_dim', 'embed_dim', 'training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_pre_hooks', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_hooks_always_called', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_state_dict_hooks', '_state_dict_pre_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', 'num_features'])\n"
     ]
    }
   ],
   "source": [
    "print(vars(model_3djepa).keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b02b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of the output od the 3Djepa encoder\n",
    "print(f\"Model output keys: {output.keys()}\")\n",
    "if 'features' in output:\n",
    "    print(f\"Features shape: {output['features'].shape}\")\n",
    "if 'points' in output:\n",
    "    print(f\"Points shape: {output['points'].shape}\")\n",
    "    \n",
    "print(\"Success! Generated embeddings from synthetic featurized scene.\")\n",
    "\n",
    "# Print some statistics about the output features\n",
    "if 'features' in output:\n",
    "    features = output['features']\n",
    "    print(f\"\\nFeature statistics:\")\n",
    "    print(f\"  Mean: {features.mean().item():.4f}\")\n",
    "    print(f\"  Std: {features.std().item():.4f}\")\n",
    "    print(f\"  Min: {features.min().item():.4f}\")\n",
    "    print(f\"  Max: {features.max().item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
