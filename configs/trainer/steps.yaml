# configs/trainer/steps.yaml
# â”€â”€â”€ core scheduling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
max_steps:          4000        # total optimiser steps
check_val_every_n_epoch: null     # ðŸ”‘ turn off epoch-end validation
val_check_interval: 1000         # validation every N steps
limit_val_batches: 0.5    # use 50 % of the validation set

# â”€â”€â”€ run one full-val at start â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
num_sanity_val_steps:     -1           # â€“1 = run the *entire* val set once before training

# â”€â”€â”€ hardware & logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
accelerator: gpu
devices: 1
precision: 16-mixed
default_root_dir: artifacts       # where PL stores checkpoints & logs
log_every_n_steps: 50
gradient_clip_val: 1.0            # leave as-is; trainer_joint.py uses .get()

# â”€â”€â”€ callbacks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
callbacks:
  - class_path: pytorch_lightning.callbacks.EarlyStopping
    init_args:
      monitor: val_loss
      patience: 4
      mode: min
  - class_path: pytorch_lightning.callbacks.ModelCheckpoint
    init_args:
      monitor: val_loss
      save_top_k: 1
      mode: min
      filename: joint-{step:06d}-{val_loss:.4f}
