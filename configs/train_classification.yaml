# ------------------------------------------------------------
# Base config – shared by every experiment
#
# for training on SSD ./data/dlrhand2_dataset_loader.sh gs://adlr2025-pointclouds/grasps/student_grasps_v1/studentGrasping_v1.tar.gz /mnt/disks/ssd 100 --mode ordered --by categories
# SSD needs all the data available for the JSON splits
#------------------------------------------------------------

# ─── checkpoint settings ────────────────────────────────────
ckpt:
  # Lightning resume-from-checkpoint (leave blank when starting fresh)
  filename: ""
  bucket: adlr2025-pointclouds       # GCS bucket
  local_dir: checkpoints             # local cache dir
  load_backbone: true    # set to true to load pre-trained backbone
  backbone_filename: pretrain_pointjepa_epoch=499-step=40500.ckpt  # remove for training from scratch

# ─── data module ────────────────────────────────────────────
data:
  preload_all: true     # eager preload
  root_dir: /mnt/disks/ssd #/studentGrasping/student_grasps_v1 # root_dir=data/dlr_hand2, when we load the whole dataset
  ssd_cache_dir: /mnt/disks/ssd/cache      # ← change from /mnt/disks/ssd/cache
  split_file: configs/splits/split_10.json   # ← choose 10 / 25 / 50 / 100
  batch_size:    32
  num_workers:   4
  num_points:    1024
  score_temp: 0.0          # set >0 for soft score-sampling

# ─── model hyper-params ────────────────────────────────────
model:
  tokenizer_num_groups: 64
  tokenizer_group_size: 32
  tokenizer_group_radius: 0.05  # None in Python
  tokenizer_unfreeze_epoch: 0
  positional_encoding_unfreeze_epoch: 0
  encoder_dim: 384
  encoder_depth: 12
  encoder_heads: 6
  encoder_dropout: 0.0
  encoder_attention_dropout: 0.0
  encoder_drop_path_rate: 0.2
  encoder_add_pos_at_every_layer: true
  encoder_qkv_bias: true
  encoder_freeze_layers: None  # None in Python
  encoder_unfreeze_layers: None  # None in Python
  encoder_unfreeze_stepwise: False
  encoder_unfreeze_stepwise_num_layers: 2
  encoder_learning_rate: 7e-4  # None in Python
  pooling_type: "attn"
  pooling_heads: 4
  pooling_dropout: 0.0
  cls_head: "mlp"
  cls_head_dim: 256
  cls_head_dropout: 0.5
  cls_head_pooling: "mean+max"
  loss_label_smoothing: 0.2
  learning_rate: 0.005
  optimizer_adamw_weight_decay: 0.05
  lr_scheduler_linear_warmup_epochs: 10
  lr_scheduler_linear_warmup_start_lr: 1e-6
  lr_scheduler_cosine_eta_min: 1e-6
  pretrained_ckpt_path: None  # None in Python
  pretrained_ckpt_ignore_encoder_layers: []  # empty list
  train_transformations:
    - "scale"
    - "center"
    - "unit_sphere"
  val_transformations:
    - "center"
    - "unit_sphere"
  transformation_scale_min: 0.8
  transformation_scale_max: 1.2
  transformation_scale_symmetries: [1, 0, 1]
  transformation_rotate_dims: [1]
  transformation_rotate_degs: null  # None in Python
  transformation_translate: 0.2
  transformation_height_normalize_dim: 1
  log_tsne: false
  log_confusion_matrix: false
  vote: false
  vote_num: 10
  encoder_unfreeze_step: 0

  encoder_unfreeze_epoch: 0
  weight_decay:          0.005          # AdamW weight-decay

# ─── logging ────────────────────────────────────────────────
logger:
  project: adlr
  entity: ADLR2025
  name: Classification - JEPA 10 

# ─── misc ───────────────────────────────────────────────────
train:
  seed: 42

hydra:
  job:
    chdir: true
