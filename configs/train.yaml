# ------------------------------------------------------------
# Base config – shared by every experiment
# ------------------------------------------------------------

# ─── checkpoint settings ────────────────────────────────────
ckpt:
  # Lightning resume-from-checkpoint (leave blank when starting fresh)
  filename: ""
  bucket: adlr2025-pointclouds       # GCS bucket
  local_dir: checkpoints             # local cache dir
  extra_epochs: 30                   # added to epochs parsed from filename

  # (optional) backbone pretrained on ShapeNet JEPA
  backbone_filename: ""              # e.g. pretrain_pointjepa_epoch=499-step=40500.ckpt

# ─── data module ────────────────────────────────────────────
data:
  root_dir: data/grasp_sample/03948459
  batch_size: 4                      # adjust when you move to GPU + SSD
  num_workers: 0
  num_points: 1024                  # must match pre-train if you load backbone

# ─── model hyper-params ────────────────────────────────────
model:
  # Tokeniser
  tokenizer_groups: 64
  tokenizer_group_size: 32
  tokenizer_radius: 0.05

  # Transformer backbone
  encoder_dim: 256
  encoder_depth: 8
  encoder_heads: 8
  encoder_dropout: 0.1
  encoder_attn_dropout: 0.1
  encoder_drop_path_rate: 0.1

  # Pooling + head
  pooling_type: mean
  pooling_heads: 4
  head_hidden_dims: [512, 256]
  head_output_dim: 1
  grasp_dim: 19

  # Learning rates
  lr_backbone: 0         # set 0 for frozen; 3e-4 to finetune
  lr_head: 3e-3

# ─── trainer args ───────────────────────────────────────────
trainer:
  default_root_dir: artifacts
  accelerator: gpu
  devices: 1
  precision: 16
  log_every_n_steps: 5
  gradient_clip_val: 1.0

# ─── logging ────────────────────────────────────────────────
logger:
  project: adlr
  entity: ADLR2025
  name: grasp_regression_run

# ─── misc ───────────────────────────────────────────────────
train:
  seed: 42
