# ------------------------------------------------------------
# Base config – shared by every experiment
#
# for training on SSD ./data/dlrhand2_dataset_loader.sh gs://adlr2025-pointclouds/grasps/student_grasps_v1/studentGrasping_v1.tar.gz /mnt/disks/ssd 100 --mode ordered --by categories
# SSD needs all the data available for the JSON splits
#------------------------------------------------------------

# ─── checkpoint settings ────────────────────────────────────
ckpt:
  # Lightning resume-from-checkpoint (leave blank when starting fresh)
  filename: ""
  bucket: adlr2025-pointclouds       # GCS bucket
  local_dir: checkpoints             # local cache dir
  backbone_filename: pretrain_pointjepa_epoch=499-step=40500.ckpt  # remove for training from scratch

# ─── data module ────────────────────────────────────────────
data:
  preload_all: true     # eager preload
  root_dir: /mnt/disks/ssd #/studentGrasping/student_grasps_v1 # root_dir=data/dlr_hand2, when we load the whole dataset
  ssd_cache_dir: /mnt/disks/ssd/cache      # ← change from /mnt/disks/ssd/cache
  split_file: configs/splits/split_25.json   # ← choose 10 / 25 / 50 / 100
  batch_size:    64
  num_workers:   4
  num_points:    1024
  score_temp: 0.0          # set >0 for soft score-sampling

# ─── model hyper-params ────────────────────────────────────
model:
  tokenizer_groups:      64
  tokenizer_group_size:  32
  tokenizer_radius:      0.05

  encoder_dim:           384 # this is fixed by the backbone
  encoder_depth:         12
  encoder_heads:         8
  encoder_dropout:       0.1
  encoder_attn_dropout:  0.1
  encoder_drop_path_rate: 0.1
  encoder_mlp_ratio:     4

  pooling_type:          att
  pooling_heads:         4
  pooling_dropout:       0.0

  head_hidden_dims:      [512, 256, 128, 12]
  num_pred:              5
  loss_type:             min_k_logit # choose between basic, min_k, min_k_logit, full

  lr_backbone:           5e-5        # 0 = frozen
  lr_head:               3e-3  
  encoder_unfreeze_epoch: 0
  weight_decay:          0.005          # AdamW weight-decay

# ─── trainer args ───────────────────────────────────────────
trainer:
  max_epochs: 10
  default_root_dir: artifacts
  accelerator: gpu
  devices: 1
  precision: 16-mixed
  log_every_n_steps: 5
  gradient_clip_val: 1.0
  # overfit_batches: 1          # sanity check for debugging: limits to 1 batch for overfit check
  callbacks:
  - class_path: pytorch_lightning.callbacks.EarlyStopping
    init_args:
      monitor: val_loss
      patience: 10           # stop if no improvement for 5 val epochs
      mode: min
  - class_path: pytorch_lightning.callbacks.ModelCheckpoint
    init_args:
      monitor: val_loss
      save_top_k: 1
      mode: min
      filename: joint-{epoch:02d}-{val_loss:.4f}
#  - class_path: callbacks.backbone_embedding_inspector.BackboneEmbeddingInspector
#    init_args:
 #     num_batches: 8

# ─── logging ────────────────────────────────────────────────
logger:
  project: adlr
  entity: ADLR2025
  name: joint_regression_mdn

# ─── misc ───────────────────────────────────────────────────
train:
  seed: 42

hydra:
  job:
    chdir: true
