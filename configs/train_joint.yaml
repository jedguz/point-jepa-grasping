# ------------------------------------------------------------
# Base config – shared by every experiment
#
# for training on SSD ./data/dlrhand2_dataset_loader.sh gs://adlr2025-pointclouds/grasps/student_grasps_v1/studentGrasping_v1.tar.gz /mnt/disks/ssd 100 --mode ordered --by categories
# SSD needs all the data available for the JSON splits
#------------------------------------------------------------

# ─── checkpoint settings ────────────────────────────────────
ckpt:
  # Lightning resume-from-checkpoint (leave blank when starting fresh)
  filename: ""
  bucket: adlr2025-pointclouds       # GCS bucket
  local_dir: checkpoints             # local cache dir
  load_backbone: true    # set to true to load pre-trained backbone
  backbone_filename: pretrain_pointjepa_epoch=499-step=40500.ckpt  # remove for training from scratch

# ─── data module ────────────────────────────────────────────
data:
  preload_all: true     # eager preload
  root_dir: /mnt/disks/ssd #/studentGrasping/student_grasps_v1 # root_dir=data/dlr_hand2, when we load the whole dataset
  ssd_cache_dir: /mnt/disks/ssd/cache      # ← change from /mnt/disks/ssd/cache
  split_file: configs/splits/split_05.json   # ← choose 10 / 25 / 50 / 100
  batch_size:    32
  num_workers:   4
  num_points:    1024
  score_temp: 0.0          # set >0 for soft score-sampling

# ─── model hyper-params ────────────────────────────────────
model:
  tokenizer_groups:      64
  tokenizer_group_size:  32
  tokenizer_radius:      0.05

  transformations:       [none]
  coord_change:          False  # set True to express pose in object frame

  encoder_dim:           384 # this is fixed by the backbone
  encoder_depth:         12
  encoder_heads:         8
  encoder_dropout:       0.1
  encoder_attn_dropout:  0.1
  encoder_drop_path_rate: 0.1
  encoder_mlp_ratio:     4

  pooling_type:          att
  pooling_heads:         4
  pooling_dropout:       0.00

  num_pred:              5
  head_hidden_dims:      [512, 256, 128]
  loss_type:             min_k_logit # choose between basic, min_k, min_k_logit, full

  # NEW: logit‑scale hyper‑params
  logit_scale_init: 0.1   # classic 0.1  (set to 0.01 to nearly disable CE)
  logit_scale_min:  0.01  # classic 0.01
  logit_scale_max:  6.0   # classic 6.0  (rarely reached)

  lr_backbone:           7e-5        # 0 = frozen
  lr_head:               1.5e-3        # 3e-3 for a basic loss
  encoder_unfreeze_epoch: 0
  weight_decay:          0.005          # AdamW weight-decay

# ─── logging ────────────────────────────────────────────────
logger:
  project: adlr
  entity: ADLR2025
  name: checkpoint_10percent

# ─── misc ───────────────────────────────────────────────────
train:
  seed: 42

hydra:
  job:
    chdir: true
