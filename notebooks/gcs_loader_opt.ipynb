{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "402ef665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nShapeNet Core GCS Loader for GCE VM Projects\\n============================================\\n\\nThis script downloads and extracts all 55 ShapeNet Core zip files from \\nGoogle Cloud Storage to your GCE VM project.\\n\\nUsage:\\n    1. Update the configuration variables below\\n    2. Run the script: python shapenet_loader.py\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ShapeNet Core GCS Loader for GCE VM Projects\n",
    "============================================\n",
    "\n",
    "This script downloads and extracts all 55 ShapeNet Core zip files from \n",
    "Google Cloud Storage to your GCE VM project.\n",
    "\n",
    "Usage:\n",
    "    1. Update the configuration variables below\n",
    "    2. Run the script: python shapenet_loader.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c32937c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from google.auth import default\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e82cc",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "CONFIGURATION - UPDATE THESE VALUES FOR YOUR PROJECT\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a4d2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"adlr2025\"           # Replace with your GCP project ID\n",
    "BUCKET_NAME = \"shapenet_bucket\"         # Replace with your GCS bucket name\n",
    "LOCAL_DATA_DIR = \"../src/data/shapenet_data\"       # Local directory for ShapeNet data\n",
    "EXTRACT_DIR = \"../src/data/shapenet_extracted\"     # Directory for extracted models\n",
    "MAX_WORKERS = 2                          # Number of parallel downloads/extractions (reduced for stability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c993b37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance tuning options\n",
    "EXTRACTION_BATCH_SIZE = 5                # How many zips to extract simultaneously\n",
    "PROGRESS_INTERVAL = 5000                 # Show progress every N files during extraction\n",
    "USE_SSD_OPTIMIZATIONS = True             # Enable optimizations for SSD storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62424329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category filtering - Set to None to extract all, or specify categories to extract\n",
    "# CATEGORIES_TO_EXTRACT = None             # Extract all categories\n",
    "CATEGORIES_TO_EXTRACT = [\"02946921\", \"02880940\", \"03085013\"]  # Only airplane, car, chair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27ae1a7d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ShapeNet Core typically has these categories (you can modify based on your bucket structure)\n",
    "SHAPENET_CATEGORIES = [\n",
    "    \"02691156\",  # airplane\n",
    "    \"02773838\",  # bag\n",
    "    \"02801938\",  # basket\n",
    "    \"02808440\",  # bathtub\n",
    "    \"02818832\",  # bed\n",
    "    \"02828884\",  # bench\n",
    "    \"02876657\",  # bottle\n",
    "    \"02880940\",  # bowl\n",
    "    \"02924116\",  # bus\n",
    "    \"02933112\",  # cabinet\n",
    "    \"02747177\",  # trash can\n",
    "    \"02942699\",  # camera\n",
    "    \"02954340\",  # cap\n",
    "    \"02958343\",  # car\n",
    "    \"03001627\",  # chair\n",
    "    \"03046257\",  # clock\n",
    "    \"03207941\",  # dishwasher\n",
    "    \"03211117\",  # display\n",
    "    \"03261776\",  # earphone\n",
    "    \"03325088\",  # faucet\n",
    "    \"03337140\",  # file cabinet\n",
    "    \"03467517\",  # guitar\n",
    "    \"03513137\",  # helmet\n",
    "    \"03593526\",  # jar\n",
    "    \"03624134\",  # knife\n",
    "    \"03636649\",  # lamp\n",
    "    \"03642806\",  # laptop\n",
    "    \"03691459\",  # loudspeaker\n",
    "    \"03710193\",  # mailbox\n",
    "    \"03759954\",  # microphone\n",
    "    \"03761084\",  # microwave\n",
    "    \"03790512\",  # motorbike\n",
    "    \"03797390\",  # mug\n",
    "    \"03928116\",  # piano\n",
    "    \"03938244\",  # pillow\n",
    "    \"03948459\",  # pistol\n",
    "    \"03991062\",  # pot\n",
    "    \"04004475\",  # printer\n",
    "    \"04074963\",  # remote control\n",
    "    \"04090263\",  # rifle\n",
    "    \"04099429\",  # rocket\n",
    "    \"04225987\",  # skateboard\n",
    "    \"04256520\",  # sofa\n",
    "    \"04330267\",  # stove\n",
    "    \"04379243\",  # table\n",
    "    \"04401088\",  # telephone\n",
    "    \"04460130\",  # tower\n",
    "    \"04468005\",  # train\n",
    "    \"04530566\",  # vessel\n",
    "    \"04554684\",  # washer\n",
    "    \"02992529\",  # cellphone\n",
    "    \"03085013\",  # keyboard\n",
    "    \"03366839\",  # folder\n",
    "    \"04401088\",  # phone\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977244a",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SETUP AND AUTHENTICATION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a5cc42e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def setup_environment():\n",
    "    \"\"\"Set up the local environment and authenticate with Google Cloud\"\"\"\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
    "    os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(\"üöÄ ShapeNet Core Loader Starting...\")\n",
    "    print(f\"Project ID: {PROJECT_ID}\")\n",
    "    print(f\"Bucket Name: {BUCKET_NAME}\")\n",
    "    print(f\"Download Directory: {LOCAL_DATA_DIR}\")\n",
    "    print(f\"Extract Directory: {EXTRACT_DIR}\")\n",
    "    print(f\"Max Workers: {MAX_WORKERS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Authenticate with Google Cloud\n",
    "    try:\n",
    "        credentials, project = default()\n",
    "        client = storage.Client(project=PROJECT_ID, credentials=credentials)\n",
    "        print(\"‚úì Successfully authenticated using default credentials\")\n",
    "        print(f\"‚úì Using project: {project}\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Authentication failed: {e}\")\n",
    "        print(\"üí° Make sure your GCE VM has the proper service account permissions\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d01bbc1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def connect_to_bucket(client, bucket_name):\n",
    "    \"\"\"Connect to the specified GCS bucket\"\"\"\n",
    "    try:\n",
    "        bucket = client.bucket(bucket_name)\n",
    "        \n",
    "        if bucket.exists():\n",
    "            print(f\"‚úì Successfully connected to bucket: {bucket_name}\")\n",
    "            bucket.reload()\n",
    "            print(f\"‚úì Bucket location: {bucket.location}\")\n",
    "            return bucket\n",
    "        else:\n",
    "            print(f\"‚ùå Bucket {bucket_name} does not exist or is not accessible\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error connecting to bucket: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c6c61",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "SHAPENET DISCOVERY FUNCTIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16f3ca71",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def discover_shapenet_zips(bucket, prefix=\"\"):\n",
    "    \"\"\"Discover all zip files in the bucket that contain ShapeNet data\"\"\"\n",
    "    print(f\"\\nüîç Discovering ShapeNet zip files...\")\n",
    "    \n",
    "    blobs = list(bucket.list_blobs(prefix=prefix))\n",
    "    zip_files = []\n",
    "    \n",
    "    for blob in blobs:\n",
    "        if blob.name.lower().endswith('.zip'):\n",
    "            size_mb = blob.size / (1024 * 1024) if blob.size else 0\n",
    "            zip_files.append({\n",
    "                'name': blob.name,\n",
    "                'size_mb': round(size_mb, 2),\n",
    "                'updated': blob.updated,\n",
    "                'blob': blob\n",
    "            })\n",
    "    \n",
    "    print(f\"üì¶ Found {len(zip_files)} zip files:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    total_size = 0\n",
    "    for zf in zip_files:\n",
    "        print(f\"üìÑ {zf['name']} ({zf['size_mb']:.2f} MB)\")\n",
    "        total_size += zf['size_mb']\n",
    "    \n",
    "    print(f\"\\nüìä Total size: {total_size:.2f} MB ({total_size/1024:.2f} GB)\")\n",
    "    \n",
    "    return zip_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b945d30",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def filter_zip_files_by_category(zip_files, categories_to_extract=None):\n",
    "    \"\"\"Filter zip files to only include specified categories\"\"\"\n",
    "    if categories_to_extract is None:\n",
    "        print(\"üì¶ Processing all categories\")\n",
    "        return zip_files\n",
    "    \n",
    "    print(f\"üéØ Filtering for categories: {categories_to_extract}\")\n",
    "    \n",
    "    filtered_files = []\n",
    "    for zip_info in zip_files:\n",
    "        # Check if any of the specified categories is in the filename\n",
    "        for category in categories_to_extract:\n",
    "            if category in zip_info['name']:\n",
    "                filtered_files.append(zip_info)\n",
    "                print(f\"  ‚úì Including: {zip_info['name']}\")\n",
    "                break\n",
    "    \n",
    "    excluded_count = len(zip_files) - len(filtered_files)\n",
    "    print(f\"üìä Filtered result: {len(filtered_files)} files selected, {excluded_count} excluded\")\n",
    "    \n",
    "    return filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccaea969",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def categorize_zip_files(zip_files):\n",
    "    \"\"\"Try to categorize zip files by ShapeNet category\"\"\"\n",
    "    categorized = {}\n",
    "    uncategorized = []\n",
    "    \n",
    "    for zf in zip_files:\n",
    "        category_found = False\n",
    "        for category in SHAPENET_CATEGORIES:\n",
    "            if category in zf['name']:\n",
    "                if category not in categorized:\n",
    "                    categorized[category] = []\n",
    "                categorized[category].append(zf)\n",
    "                category_found = True\n",
    "                break\n",
    "        \n",
    "        if not category_found:\n",
    "            uncategorized.append(zf)\n",
    "    \n",
    "    print(f\"\\nüìã Categorization Summary:\")\n",
    "    print(f\"  ‚Ä¢ Categorized: {len(categorized)} categories\")\n",
    "    print(f\"  ‚Ä¢ Uncategorized: {len(uncategorized)} files\")\n",
    "    \n",
    "    return categorized, uncategorized\n",
    "    \"\"\"Try to categorize zip files by ShapeNet category\"\"\"\n",
    "    categorized = {}\n",
    "    uncategorized = []\n",
    "    \n",
    "    for zf in zip_files:\n",
    "        category_found = False\n",
    "        for category in SHAPENET_CATEGORIES:\n",
    "            if category in zf['name']:\n",
    "                if category not in categorized:\n",
    "                    categorized[category] = []\n",
    "                categorized[category].append(zf)\n",
    "                category_found = True\n",
    "                break\n",
    "        \n",
    "        if not category_found:\n",
    "            uncategorized.append(zf)\n",
    "    \n",
    "    print(f\"\\nüìã Categorization Summary:\")\n",
    "    print(f\"  ‚Ä¢ Categorized: {len(categorized)} categories\")\n",
    "    print(f\"  ‚Ä¢ Uncategorized: {len(uncategorized)} files\")\n",
    "    \n",
    "    return categorized, uncategorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef8455",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "DOWNLOAD FUNCTIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19c0cd70",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def download_zip_file(bucket, zip_info, local_dir, progress_callback=None):\n",
    "    \"\"\"Download a single zip file\"\"\"\n",
    "    blob_name = zip_info['name']\n",
    "    local_path = os.path.join(local_dir, os.path.basename(blob_name))\n",
    "    \n",
    "    try:\n",
    "        blob = bucket.blob(blob_name)\n",
    "        \n",
    "        # Check if file already exists\n",
    "        if os.path.exists(local_path):\n",
    "            local_size = os.path.getsize(local_path)\n",
    "            remote_size = blob.size\n",
    "            if local_size == remote_size:\n",
    "                print(f\"‚è≠Ô∏è  Skipping {blob_name} (already exists)\")\n",
    "                return local_path, True\n",
    "        \n",
    "        print(f\"‚¨áÔ∏è  Downloading {blob_name} ({zip_info['size_mb']:.2f} MB)...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        blob.download_to_filename(local_path)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        speed = zip_info['size_mb'] / elapsed if elapsed > 0 else 0\n",
    "        \n",
    "        print(f\"‚úì Downloaded {blob_name} in {elapsed:.1f}s ({speed:.1f} MB/s)\")\n",
    "        \n",
    "        if progress_callback:\n",
    "            progress_callback(blob_name, True)\n",
    "            \n",
    "        return local_path, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading {blob_name}: {e}\")\n",
    "        if progress_callback:\n",
    "            progress_callback(blob_name, False)\n",
    "        return None, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c2b30af",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def download_all_zips(bucket, zip_files, local_dir, max_workers=MAX_WORKERS):\n",
    "    \"\"\"Download all zip files using parallel workers\"\"\"\n",
    "    print(f\"\\n‚¨áÔ∏è  Starting download of {len(zip_files)} files using {max_workers} workers...\")\n",
    "    \n",
    "    downloaded_files = []\n",
    "    failed_downloads = []\n",
    "    \n",
    "    # Progress tracking\n",
    "    completed = threading.Event()\n",
    "    progress_lock = threading.Lock()\n",
    "    progress_data = {'completed': 0, 'total': len(zip_files), 'failed': 0}\n",
    "    \n",
    "    def progress_callback(filename, success):\n",
    "        with progress_lock:\n",
    "            progress_data['completed'] += 1\n",
    "            if not success:\n",
    "                progress_data['failed'] += 1\n",
    "            print(f\"üìä Progress: {progress_data['completed']}/{progress_data['total']} \"\n",
    "                  f\"(Failed: {progress_data['failed']})\")\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel downloads\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all download tasks\n",
    "        future_to_zip = {\n",
    "            executor.submit(download_zip_file, bucket, zf, local_dir, progress_callback): zf \n",
    "            for zf in zip_files\n",
    "        }\n",
    "        \n",
    "        # Collect results\n",
    "        for future in as_completed(future_to_zip):\n",
    "            zip_info = future_to_zip[future]\n",
    "            try:\n",
    "                local_path, success = future.result()\n",
    "                if success and local_path:\n",
    "                    downloaded_files.append(local_path)\n",
    "                else:\n",
    "                    failed_downloads.append(zip_info['name'])\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Download failed for {zip_info['name']}: {e}\")\n",
    "                failed_downloads.append(zip_info['name'])\n",
    "    \n",
    "    print(f\"\\nüì¶ Download Summary:\")\n",
    "    print(f\"  ‚úì Successfully downloaded: {len(downloaded_files)}\")\n",
    "    print(f\"  ‚ùå Failed downloads: {len(failed_downloads)}\")\n",
    "    \n",
    "    if failed_downloads:\n",
    "        print(f\"  Failed files: {failed_downloads}\")\n",
    "    \n",
    "    return downloaded_files, failed_downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e64bb",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "EXTRACTION FUNCTIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfb73f3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_zip_file(zip_path, extract_dir, preserve_structure=True, progress_interval=5000):\n",
    "    \"\"\"Extract a single zip file with optimized performance\"\"\"\n",
    "    try:\n",
    "        zip_name = os.path.basename(zip_path)\n",
    "        print(f\"üìÇ Extracting {zip_name}...\")\n",
    "        \n",
    "        # Create extraction subdirectory if preserving structure\n",
    "        if preserve_structure:\n",
    "            extract_subdir = os.path.join(extract_dir, os.path.splitext(zip_name)[0])\n",
    "            os.makedirs(extract_subdir, exist_ok=True)\n",
    "            final_extract_dir = extract_subdir\n",
    "        else:\n",
    "            final_extract_dir = extract_dir\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            # Get info about the zip\n",
    "            file_list = zip_ref.infolist()\n",
    "            file_count = len(file_list)\n",
    "            \n",
    "            print(f\"  üìÑ {file_count} files to extract...\")\n",
    "            \n",
    "            # Extract files with progress updates for large archives\n",
    "            if file_count > 1000:\n",
    "                extracted = 0\n",
    "                last_update = 0\n",
    "                \n",
    "                for file_info in file_list:\n",
    "                    # Skip directories\n",
    "                    if file_info.is_dir():\n",
    "                        continue\n",
    "                        \n",
    "                    # Extract individual file\n",
    "                    zip_ref.extract(file_info, final_extract_dir)\n",
    "                    extracted += 1\n",
    "                    \n",
    "                    # Progress update every N files\n",
    "                    if extracted - last_update >= progress_interval:\n",
    "                        percent = (extracted / file_count) * 100\n",
    "                        elapsed_so_far = time.time() - start_time\n",
    "                        rate = extracted / elapsed_so_far if elapsed_so_far > 0 else 0\n",
    "                        eta = (file_count - extracted) / rate if rate > 0 else 0\n",
    "                        \n",
    "                        print(f\"    Progress: {extracted}/{file_count} ({percent:.1f}%) \"\n",
    "                              f\"[{rate:.0f} files/s, ETA: {eta/60:.1f}m]\")\n",
    "                        last_update = extracted\n",
    "            else:\n",
    "                # For smaller archives, extract normally\n",
    "                zip_ref.extractall(final_extract_dir)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        rate = file_count / elapsed if elapsed > 0 else 0\n",
    "        print(f\"‚úì Extracted {zip_name} ({file_count} files) in {elapsed:.1f}s ({rate:.0f} files/s)\")\n",
    "        \n",
    "        return final_extract_dir, file_count, True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting {zip_path}: {e}\")\n",
    "        return None, 0, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65951c35",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_all_zips(zip_files, extract_dir, max_workers=EXTRACTION_BATCH_SIZE, preserve_structure=True):\n",
    "    \"\"\"Extract all zip files using parallel workers with performance optimizations\"\"\"\n",
    "    print(f\"\\nüìÇ Starting extraction of {len(zip_files)} files...\")\n",
    "    print(f\"   Using {max_workers} parallel extractions\")\n",
    "    print(f\"   Progress updates every {PROGRESS_INTERVAL} files\")\n",
    "    \n",
    "    extracted_dirs = []\n",
    "    failed_extractions = []\n",
    "    total_files = 0\n",
    "    \n",
    "    # Sort zip files by size (smallest first for better load balancing)\n",
    "    zip_files_sorted = sorted(zip_files, key=lambda x: os.path.getsize(x) if os.path.exists(x) else 0)\n",
    "    \n",
    "    # Progress tracking\n",
    "    progress_lock = threading.Lock()\n",
    "    progress_data = {'completed': 0, 'total': len(zip_files), 'failed': 0}\n",
    "    \n",
    "    def extract_with_progress(zip_path):\n",
    "        result_dir, file_count, success = extract_zip_file(\n",
    "            zip_path, extract_dir, preserve_structure, PROGRESS_INTERVAL\n",
    "        )\n",
    "        \n",
    "        with progress_lock:\n",
    "            progress_data['completed'] += 1\n",
    "            if not success:\n",
    "                progress_data['failed'] += 1\n",
    "            \n",
    "            remaining = progress_data['total'] - progress_data['completed']\n",
    "            print(f\"üìä Overall Progress: {progress_data['completed']}/{progress_data['total']} \"\n",
    "                  f\"({remaining} remaining, {progress_data['failed']} failed)\")\n",
    "        \n",
    "        return result_dir, file_count, success, zip_path\n",
    "    \n",
    "    # Use ThreadPoolExecutor with reduced workers for extraction\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all extraction tasks\n",
    "        futures = [executor.submit(extract_with_progress, zip_path) for zip_path in zip_files_sorted]\n",
    "        \n",
    "        # Collect results\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result_dir, file_count, success, zip_path = future.result()\n",
    "                if success:\n",
    "                    extracted_dirs.append(result_dir)\n",
    "                    total_files += file_count\n",
    "                else:\n",
    "                    failed_extractions.append(zip_path)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Extraction failed: {e}\")\n",
    "                failed_extractions.append(\"unknown\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Extraction Summary:\")\n",
    "    print(f\"  ‚úì Successfully extracted: {len(extracted_dirs)} archives\")\n",
    "    print(f\"  üìÑ Total files extracted: {total_files:,}\")\n",
    "    print(f\"  ‚ùå Failed extractions: {len(failed_extractions)}\")\n",
    "    \n",
    "    return extracted_dirs, failed_extractions, total_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181539ea",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "ANALYSIS FUNCTIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a143f19f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_shapenet_structure(extract_dir):\n",
    "    \"\"\"Analyze the structure of extracted ShapeNet data\"\"\"\n",
    "    print(f\"\\nüîç Analyzing ShapeNet data structure in {extract_dir}...\")\n",
    "    \n",
    "    structure_info = {}\n",
    "    total_models = 0\n",
    "    \n",
    "    extract_path = Path(extract_dir)\n",
    "    \n",
    "    for category_dir in extract_path.iterdir():\n",
    "        if category_dir.is_dir():\n",
    "            category_name = category_dir.name\n",
    "            \n",
    "            # Count models in this category\n",
    "            model_count = 0\n",
    "            model_dirs = []\n",
    "            \n",
    "            for item in category_dir.rglob('*'):\n",
    "                if item.is_dir() and len(item.name) == 32:  # ShapeNet model IDs are 32 chars\n",
    "                    model_count += 1\n",
    "                    model_dirs.append(item)\n",
    "            \n",
    "            structure_info[category_name] = {\n",
    "                'model_count': model_count,\n",
    "                'path': str(category_dir),\n",
    "                'model_dirs': model_dirs[:5]  # Store first 5 for sampling\n",
    "            }\n",
    "            \n",
    "            total_models += model_count\n",
    "    \n",
    "    print(f\"\\nüìä ShapeNet Structure Analysis:\")\n",
    "    print(f\"  üìÇ Categories found: {len(structure_info)}\")\n",
    "    print(f\"  üéØ Total models: {total_models}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for category, info in structure_info.items():\n",
    "        print(f\"  {category}: {info['model_count']} models\")\n",
    "    \n",
    "    return structure_info, total_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a960c6a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def sample_model_files(structure_info, sample_size=3):\n",
    "    \"\"\"Sample some model files to understand the data format\"\"\"\n",
    "    print(f\"\\nüî¨ Sampling model files (up to {sample_size} per category)...\")\n",
    "    \n",
    "    file_types = {}\n",
    "    \n",
    "    for category, info in structure_info.items():\n",
    "        if info['model_dirs']:\n",
    "            print(f\"\\nüìÇ Category: {category}\")\n",
    "            \n",
    "            for i, model_dir in enumerate(info['model_dirs'][:sample_size]):\n",
    "                print(f\"  Model {i+1}: {model_dir.name}\")\n",
    "                \n",
    "                # List files in this model directory\n",
    "                for file_path in model_dir.iterdir():\n",
    "                    if file_path.is_file():\n",
    "                        ext = file_path.suffix.lower()\n",
    "                        if ext not in file_types:\n",
    "                            file_types[ext] = 0\n",
    "                        file_types[ext] += 1\n",
    "                        print(f\"    üìÑ {file_path.name}\")\n",
    "    \n",
    "    print(f\"\\nüìã File type summary:\")\n",
    "    for ext, count in sorted(file_types.items()):\n",
    "        print(f\"  {ext}: {count} files\")\n",
    "    \n",
    "    return file_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd7940",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "UTILITY FUNCTIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68b54b36",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def cleanup_zip_files(zip_files, keep_zips=False):\n",
    "    \"\"\"Clean up downloaded zip files after extraction\"\"\"\n",
    "    if keep_zips:\n",
    "        print(\"\\nüíæ Keeping zip files as requested\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüßπ Cleaning up {len(zip_files)} zip files...\")\n",
    "    \n",
    "    removed_count = 0\n",
    "    for zip_path in zip_files:\n",
    "        try:\n",
    "            if os.path.exists(zip_path):\n",
    "                os.remove(zip_path)\n",
    "                removed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error removing {zip_path}: {e}\")\n",
    "    \n",
    "    print(f\"‚úì Removed {removed_count} zip files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bce975f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_shapenet_manifest(structure_info, total_models, extract_dir):\n",
    "    \"\"\"Save a manifest of the ShapeNet data\"\"\"\n",
    "    manifest = {\n",
    "        \"total_categories\": len(structure_info),\n",
    "        \"total_models\": total_models,\n",
    "        \"extract_directory\": extract_dir,\n",
    "        \"categories\": {}\n",
    "    }\n",
    "    \n",
    "    for category, info in structure_info.items():\n",
    "        manifest[\"categories\"][category] = {\n",
    "            \"model_count\": info[\"model_count\"],\n",
    "            \"path\": info[\"path\"]\n",
    "        }\n",
    "    \n",
    "    manifest_path = os.path.join(extract_dir, \"shapenet_manifest.json\")\n",
    "    with open(manifest_path, 'w') as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    \n",
    "    print(f\"üìÑ Manifest saved to: {manifest_path}\")\n",
    "    return manifest_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e4a63a",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "MAIN EXECUTION FUNCTION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8744285",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the ShapeNet loading process\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üéØ SHAPENET CORE LOADER\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Setup and authentication\n",
    "    client = setup_environment()\n",
    "    if not client:\n",
    "        return\n",
    "    \n",
    "    # Step 2: Connect to bucket\n",
    "    bucket = connect_to_bucket(client, BUCKET_NAME)\n",
    "    if not bucket:\n",
    "        return\n",
    "    \n",
    "    # Step 3: Discover ShapeNet zip files\n",
    "    zip_files = discover_shapenet_zips(bucket)\n",
    "    if not zip_files:\n",
    "        print(\"‚ùå No zip files found in the bucket!\")\n",
    "        return\n",
    "    \n",
    "    # Step 4: Show categorization\n",
    "    categorized, uncategorized = categorize_zip_files(zip_files)\n",
    "    \n",
    "    # Step 4.5: Filter categories if specified\n",
    "    if CATEGORIES_TO_EXTRACT is not None:\n",
    "        zip_files = filter_zip_files_by_category(zip_files, CATEGORIES_TO_EXTRACT)\n",
    "        if not zip_files:\n",
    "            print(\"‚ùå No files match the specified categories!\")\n",
    "            return\n",
    "    \n",
    "    # Step 5: Confirm download\n",
    "    total_size_gb = sum(zf['size_mb'] for zf in zip_files) / 1024\n",
    "    print(f\"\\n‚ùì Ready to download {len(zip_files)} files ({total_size_gb:.2f} GB)?\")\n",
    "    print(f\"   This will use approximately {total_size_gb * 2:.1f} GB of disk space (zip + extracted)\")\n",
    "    \n",
    "    confirm = input(\"Continue? (y/n): \").strip().lower()\n",
    "    if confirm != 'y':\n",
    "        print(\"‚ùå Download cancelled\")\n",
    "        return\n",
    "    \n",
    "    # Step 6: Download all zip files\n",
    "    downloaded_files, failed_downloads = download_all_zips(bucket, zip_files, LOCAL_DATA_DIR)\n",
    "    \n",
    "    if not downloaded_files:\n",
    "        print(\"‚ùå No files were downloaded successfully!\")\n",
    "        return\n",
    "    \n",
    "    # Step 7: Extract all zip files\n",
    "    print(f\"\\nüîÑ Starting extraction phase...\")\n",
    "    keep_zips = input(\"Keep zip files after extraction? (y/n): \").strip().lower() == 'y'\n",
    "    \n",
    "    extracted_dirs, failed_extractions, total_files = extract_all_zips(\n",
    "        downloaded_files, EXTRACT_DIR, preserve_structure=True\n",
    "    )\n",
    "    \n",
    "    # Step 8: Analyze structure\n",
    "    structure_info, total_models = analyze_shapenet_structure(EXTRACT_DIR)\n",
    "    \n",
    "    # Step 9: Sample files\n",
    "    if structure_info:\n",
    "        sample_files = input(\"Sample model files to understand structure? (y/n): \").strip().lower() == 'y'\n",
    "        if sample_files:\n",
    "            file_types = sample_model_files(structure_info)\n",
    "    \n",
    "    # Step 10: Save manifest\n",
    "    manifest_path = save_shapenet_manifest(structure_info, total_models, EXTRACT_DIR)\n",
    "    \n",
    "    # Step 11: Cleanup\n",
    "    if not keep_zips:\n",
    "        cleanup_zip_files(downloaded_files, keep_zips=False)\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nüéâ SHAPENET CORE LOADING COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìä Summary:\")\n",
    "    print(f\"  ‚Ä¢ Downloaded: {len(downloaded_files)} zip files\")\n",
    "    print(f\"  ‚Ä¢ Extracted: {len(extracted_dirs)} archives\")\n",
    "    print(f\"  ‚Ä¢ Total files: {total_files}\")\n",
    "    print(f\"  ‚Ä¢ Categories: {len(structure_info)}\")\n",
    "    print(f\"  ‚Ä¢ Total models: {total_models}\")\n",
    "    print(f\"  ‚Ä¢ Data location: {EXTRACT_DIR}\")\n",
    "    print(f\"  ‚Ä¢ Manifest: {manifest_path}\")\n",
    "    \n",
    "    if failed_downloads:\n",
    "        print(f\"  ‚ö†Ô∏è  Failed downloads: {len(failed_downloads)}\")\n",
    "    if failed_extractions:\n",
    "        print(f\"  ‚ö†Ô∏è  Failed extractions: {len(failed_extractions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55d6b647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Before running, make sure to update:\n",
      "   ‚Ä¢ PROJECT_ID: adlr2025\n",
      "   ‚Ä¢ BUCKET_NAME: shapenet_bucket\n",
      "\n",
      "‚ñ∂Ô∏è  Starting ShapeNet Core loader...\n",
      "============================================================\n",
      "üéØ SHAPENET CORE LOADER\n",
      "============================================================\n",
      "üöÄ ShapeNet Core Loader Starting...\n",
      "Project ID: adlr2025\n",
      "Bucket Name: shapenet_bucket\n",
      "Download Directory: ../src/data/shapenet_data\n",
      "Extract Directory: ../src/data/shapenet_extracted\n",
      "Max Workers: 2\n",
      "------------------------------------------------------------\n",
      "‚úì Successfully authenticated using default credentials\n",
      "‚úì Using project: adlr2025\n",
      "‚úì Successfully connected to bucket: shapenet_bucket\n",
      "‚úì Bucket location: EUROPE-WEST3\n",
      "\n",
      "üîç Discovering ShapeNet zip files...\n",
      "üì¶ Found 55 zip files:\n",
      "--------------------------------------------------------------------------------\n",
      "üìÑ 02691156.zip (3203.42 MB)\n",
      "üìÑ 02747177.zip (61.14 MB)\n",
      "üìÑ 02773838.zip (27.64 MB)\n",
      "üìÑ 02801938.zip (30.65 MB)\n",
      "üìÑ 02808440.zip (193.99 MB)\n",
      "üìÑ 02818832.zip (69.93 MB)\n",
      "üìÑ 02828884.zip (359.33 MB)\n",
      "üìÑ 02843684.zip (4.21 MB)\n",
      "üìÑ 02871439.zip (74.00 MB)\n",
      "üìÑ 02876657.zip (85.31 MB)\n",
      "üìÑ 02880940.zip (20.44 MB)\n",
      "üìÑ 02924116.zip (692.97 MB)\n",
      "üìÑ 02933112.zip (412.70 MB)\n",
      "üìÑ 02942699.zip (32.26 MB)\n",
      "üìÑ 02946921.zip (19.05 MB)\n",
      "üìÑ 02954340.zip (42.52 MB)\n",
      "üìÑ 02958343.zip (5422.08 MB)\n",
      "üìÑ 02992529.zip (227.39 MB)\n",
      "üìÑ 03001627.zip (1874.23 MB)\n",
      "üìÑ 03046257.zip (177.57 MB)\n",
      "üìÑ 03085013.zip (18.12 MB)\n",
      "üìÑ 03207941.zip (10.36 MB)\n",
      "üìÑ 03211117.zip (272.26 MB)\n",
      "üìÑ 03261776.zip (22.32 MB)\n",
      "üìÑ 03325088.zip (149.38 MB)\n",
      "üìÑ 03337140.zip (73.16 MB)\n",
      "üìÑ 03467517.zip (553.55 MB)\n",
      "üìÑ 03513137.zip (36.93 MB)\n",
      "üìÑ 03593526.zip (162.90 MB)\n",
      "üìÑ 03624134.zip (61.49 MB)\n",
      "üìÑ 03636649.zip (710.79 MB)\n",
      "üìÑ 03642806.zip (218.79 MB)\n",
      "üìÑ 03691459.zip (504.89 MB)\n",
      "üìÑ 03710193.zip (10.84 MB)\n",
      "üìÑ 03759954.zip (13.94 MB)\n",
      "üìÑ 03761084.zip (33.95 MB)\n",
      "üìÑ 03790512.zip (545.12 MB)\n",
      "üìÑ 03797390.zip (23.50 MB)\n",
      "üìÑ 03928116.zip (84.00 MB)\n",
      "üìÑ 03938244.zip (27.04 MB)\n",
      "üìÑ 03948459.zip (81.06 MB)\n",
      "üìÑ 03991062.zip (298.55 MB)\n",
      "üìÑ 04004475.zip (31.85 MB)\n",
      "üìÑ 04074963.zip (10.36 MB)\n",
      "üìÑ 04090263.zip (884.97 MB)\n",
      "üìÑ 04099429.zip (21.16 MB)\n",
      "üìÑ 04225987.zip (79.85 MB)\n",
      "üìÑ 04256520.zip (1232.10 MB)\n",
      "üìÑ 04330267.zip (79.45 MB)\n",
      "üìÑ 04379243.zip (1589.76 MB)\n",
      "üìÑ 04401088.zip (295.08 MB)\n",
      "üìÑ 04460130.zip (60.40 MB)\n",
      "üìÑ 04468005.zip (353.43 MB)\n",
      "üìÑ 04530566.zip (1251.94 MB)\n",
      "üìÑ 04554684.zip (25.41 MB)\n",
      "\n",
      "üìä Total size: 22859.53 MB (22.32 GB)\n",
      "\n",
      "üìã Categorization Summary:\n",
      "  ‚Ä¢ Categorized: 52 categories\n",
      "  ‚Ä¢ Uncategorized: 3 files\n",
      "üéØ Filtering for categories: ['02946921', '02880940', '03085013']\n",
      "  ‚úì Including: 02880940.zip\n",
      "  ‚úì Including: 02946921.zip\n",
      "  ‚úì Including: 03085013.zip\n",
      "üìä Filtered result: 3 files selected, 52 excluded\n",
      "\n",
      "‚ùì Ready to download 3 files (0.06 GB)?\n",
      "   This will use approximately 0.1 GB of disk space (zip + extracted)\n",
      "\n",
      "‚¨áÔ∏è  Starting download of 3 files using 2 workers...\n",
      "‚¨áÔ∏è  Downloading 02880940.zip (20.44 MB)...\n",
      "‚¨áÔ∏è  Downloading 02946921.zip (19.05 MB)...\n",
      "‚úì Downloaded 02880940.zip in 0.5s (44.9 MB/s)\n",
      "üìä Progress: 1/3 (Failed: 0)\n",
      "‚¨áÔ∏è  Downloading 03085013.zip (18.12 MB)...\n",
      "‚úì Downloaded 02946921.zip in 0.5s (39.4 MB/s)\n",
      "üìä Progress: 2/3 (Failed: 0)\n",
      "‚úì Downloaded 03085013.zip in 0.3s (67.5 MB/s)\n",
      "üìä Progress: 3/3 (Failed: 0)\n",
      "\n",
      "üì¶ Download Summary:\n",
      "  ‚úì Successfully downloaded: 3\n",
      "  ‚ùå Failed downloads: 0\n",
      "\n",
      "üîÑ Starting extraction phase...\n",
      "\n",
      "üìÇ Starting extraction of 3 files...\n",
      "   Using 5 parallel extractions\n",
      "   Progress updates every 5000 files\n",
      "üìÇ Extracting 03085013.zip...\n",
      "üìÇ Extracting 02946921.zip...\n",
      "üìÇ Extracting 02880940.zip...\n",
      "  üìÑ 525 files to extract...\n",
      "  üìÑ 1814 files to extract...\n",
      "  üìÑ 944 files to extract...\n",
      "‚úì Extracted 02946921.zip (944 files) in 1.1s (892 files/s)\n",
      "üìä Overall Progress: 1/3 (2 remaining, 0 failed)\n",
      "‚úì Extracted 03085013.zip (525 files) in 1.2s (431 files/s)\n",
      "üìä Overall Progress: 2/3 (1 remaining, 0 failed)\n",
      "‚úì Extracted 02880940.zip (1814 files) in 1.5s (1235 files/s)\n",
      "üìä Overall Progress: 3/3 (0 remaining, 0 failed)\n",
      "\n",
      "üìÅ Extraction Summary:\n",
      "  ‚úì Successfully extracted: 3 archives\n",
      "  üìÑ Total files extracted: 3,283\n",
      "  ‚ùå Failed extractions: 0\n",
      "\n",
      "üîç Analyzing ShapeNet data structure in ../src/data/shapenet_extracted...\n",
      "\n",
      "üìä ShapeNet Structure Analysis:\n",
      "  üìÇ Categories found: 6\n",
      "  üéØ Total models: 12976\n",
      "------------------------------------------------------------\n",
      "  02958343: 3093 models\n",
      "  02880940: 167 models\n",
      "  03001627: 6005 models\n",
      "  02691156: 3567 models\n",
      "  02946921: 88 models\n",
      "  03085013: 56 models\n",
      "üìÑ Manifest saved to: ../src/data/shapenet_extracted/shapenet_manifest.json\n",
      "\n",
      "üßπ Cleaning up 3 zip files...\n",
      "‚úì Removed 3 zip files\n",
      "\n",
      "üéâ SHAPENET CORE LOADING COMPLETE!\n",
      "============================================================\n",
      "üìä Summary:\n",
      "  ‚Ä¢ Downloaded: 3 zip files\n",
      "  ‚Ä¢ Extracted: 3 archives\n",
      "  ‚Ä¢ Total files: 3283\n",
      "  ‚Ä¢ Categories: 6\n",
      "  ‚Ä¢ Total models: 12976\n",
      "  ‚Ä¢ Data location: ../src/data/shapenet_extracted\n",
      "  ‚Ä¢ Manifest: ../src/data/shapenet_extracted/shapenet_manifest.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"üí° Before running, make sure to update:\")\n",
    "    print(f\"   ‚Ä¢ PROJECT_ID: {PROJECT_ID}\")\n",
    "    print(f\"   ‚Ä¢ BUCKET_NAME: {BUCKET_NAME}\")\n",
    "    print(\"\\n‚ñ∂Ô∏è  Starting ShapeNet Core loader...\")\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
