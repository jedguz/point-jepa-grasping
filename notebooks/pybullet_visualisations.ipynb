{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "605c131f",
   "metadata": {},
   "source": [
    "## Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddd28f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from scripts.joint_regressor import JointRegressor\n",
    "from scripts.checkpoint_utils import load_full_checkpoint\n",
    "from scripts.dlrhand2_joint_datamodule import _sample_mesh_as_pc\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    joint_ckpt: str,\n",
    "    config_path: str,\n",
    "    backbone_ckpt: str | None = None,\n",
    "    device: str = \"cuda:0\"\n",
    ") -> JointRegressor:\n",
    "    \"\"\"Instantiate JointRegressor, optionally load a JEPA backbone, then load trained joint-ckpt weights.\"\"\"\n",
    "    # Load training config\n",
    "    cfg = OmegaConf.load(config_path)\n",
    "\n",
    "    # Build model with training hyperparams\n",
    "    model = JointRegressor(\n",
    "        num_points=cfg.data.num_points,\n",
    "        tokenizer_groups=cfg.model.tokenizer_groups,\n",
    "        tokenizer_group_size=cfg.model.tokenizer_group_size,\n",
    "        tokenizer_radius=cfg.model.tokenizer_radius,\n",
    "        encoder_dim=cfg.model.encoder_dim,\n",
    "        encoder_depth=cfg.model.encoder_depth,\n",
    "        encoder_heads=cfg.model.encoder_heads,\n",
    "        encoder_dropout=cfg.model.encoder_dropout,\n",
    "        encoder_attn_dropout=cfg.model.encoder_attn_dropout,\n",
    "        encoder_drop_path_rate=cfg.model.encoder_drop_path_rate,\n",
    "        encoder_mlp_ratio=cfg.model.encoder_mlp_ratio,\n",
    "        pooling_type=cfg.model.pooling_type,\n",
    "        pooling_heads=cfg.model.pooling_heads,\n",
    "        pooling_dropout=cfg.model.pooling_dropout,\n",
    "        head_hidden_dims=cfg.model.head_hidden_dims,\n",
    "        lr_backbone=cfg.model.lr_backbone,\n",
    "        lr_head=cfg.model.lr_head,\n",
    "    )\n",
    "\n",
    "    # Load backbone pretrain if provided\n",
    "    if backbone_ckpt:\n",
    "        load_full_checkpoint(model, backbone_ckpt)\n",
    "\n",
    "    # Load full joint-regression checkpoint (overwrites head + backbone)\n",
    "    checkpoint = torch.load(joint_ckpt, map_location=device)\n",
    "    state = checkpoint.get('state_dict', checkpoint)\n",
    "    model.load_state_dict(state, strict=False)\n",
    "\n",
    "    model.eval()\n",
    "    model.freeze()\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_joint_angles(\n",
    "    model: JointRegressor,\n",
    "    mesh_path: str,\n",
    "    pose7d: list[float] | np.ndarray,\n",
    "    num_points: int | None = None,\n",
    "    device: str = \"cuda:0\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given a mesh file and a 7D hand pose, returns the 12-D joint-angle prediction.\n",
    "    \"\"\"\n",
    "    n_pts = num_points or model.hparams.num_points\n",
    "    pc = _sample_mesh_as_pc(mesh_path, n=n_pts)\n",
    "\n",
    "    pts = torch.from_numpy(pc).unsqueeze(0).to(device)\n",
    "    pose = torch.as_tensor(pose7d, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    pred = model(pts, pose)  # (1,12)\n",
    "    return pred.squeeze(0).cpu().numpy()\n",
    "\n",
    "# Example usage:\n",
    "# model = load_model(\n",
    "#     joint_ckpt=\"checkpoints/joint-final.ckpt\",\n",
    "#     config_path=\"configs/train_joint.yaml\",\n",
    "#     backbone_ckpt=\"checkpoints/pretrain-pointjepa.ckpt\",  # optional\n",
    "#     device=\"cuda:0\"\n",
    "# )\n",
    "# angles = predict_joint_angles(model, mesh_path, hand_pose)\n",
    "# print(angles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97e8d462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted joint angles: [-0.149     0.1322   -0.2136    0.09985  -0.1733   -0.04956   0.03072\n",
      " -0.09595   0.008736  0.028    -0.10754  -0.02014 ]\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\n",
    "    joint_ckpt=\"../configs/checkpoints/jepa_no_FT.ckpt\",            # your trained regressor\n",
    "    config_path=\"../configs/train_joint.yaml\",\n",
    "    #backbone_ckpt=\"checkpoints/pretrain_pointjepa.ckpt\",  # if you want to override backbone first\n",
    "    device=\"cuda:0\",\n",
    ")\n",
    "\n",
    "angles = predict_joint_angles(\n",
    "    model,\n",
    "    mesh_path=\"../data/grasp_sample/02818832/4bc7ad3dbb8fc8747d8864caa856253b/0/mesh.obj\",\n",
    "    pose7d=[0.035, -0.01, 0.07, 0.0, 0.0, 0.0, 1.0],\n",
    ")\n",
    "print(\"Predicted joint angles:\", angles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf1c84",
   "metadata": {},
   "source": [
    "## Helper: Checkpoint Inspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ec09628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_checkpoint(\n",
    "    ckpt_path: str,\n",
    "    prefix_filter: str | None = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Print all parameter keys saved in a checkpoint. Optionally filter by prefix.\n",
    "\n",
    "    Args:\n",
    "        ckpt_path: path to the .ckpt or .pth file\n",
    "        prefix_filter: only show keys containing this substring\n",
    "    \"\"\"\n",
    "    # Load checkpoint on CPU\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "    state = ckpt.get('state_dict', ckpt)\n",
    "    keys = list(state.keys())\n",
    "\n",
    "    if prefix_filter:\n",
    "        keys = [k for k in keys if prefix_filter in k]\n",
    "\n",
    "    print(f\"Found {len(keys)} parameters{' with filter ' + prefix_filter if prefix_filter else ''}:\")\n",
    "    for k in sorted(keys):\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10ba6289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 546 parameters:\n",
      "mask_token\n",
      "positional_encoding.0.bias\n",
      "positional_encoding.0.weight\n",
      "positional_encoding.2.bias\n",
      "positional_encoding.2.weight\n",
      "predictor.mask_token\n",
      "predictor.positional_encoding.0.bias\n",
      "predictor.positional_encoding.0.weight\n",
      "predictor.positional_encoding.2.bias\n",
      "predictor.positional_encoding.2.weight\n",
      "predictor.predictor.blocks.0.attn.proj.bias\n",
      "predictor.predictor.blocks.0.attn.proj.weight\n",
      "predictor.predictor.blocks.0.attn.qkv.bias\n",
      "predictor.predictor.blocks.0.attn.qkv.weight\n",
      "predictor.predictor.blocks.0.mlp.fc1.bias\n",
      "predictor.predictor.blocks.0.mlp.fc1.weight\n",
      "predictor.predictor.blocks.0.mlp.fc2.bias\n",
      "predictor.predictor.blocks.0.mlp.fc2.weight\n",
      "predictor.predictor.blocks.0.norm1.bias\n",
      "predictor.predictor.blocks.0.norm1.weight\n",
      "predictor.predictor.blocks.0.norm2.bias\n",
      "predictor.predictor.blocks.0.norm2.weight\n",
      "predictor.predictor.blocks.1.attn.proj.bias\n",
      "predictor.predictor.blocks.1.attn.proj.weight\n",
      "predictor.predictor.blocks.1.attn.qkv.bias\n",
      "predictor.predictor.blocks.1.attn.qkv.weight\n",
      "predictor.predictor.blocks.1.mlp.fc1.bias\n",
      "predictor.predictor.blocks.1.mlp.fc1.weight\n",
      "predictor.predictor.blocks.1.mlp.fc2.bias\n",
      "predictor.predictor.blocks.1.mlp.fc2.weight\n",
      "predictor.predictor.blocks.1.norm1.bias\n",
      "predictor.predictor.blocks.1.norm1.weight\n",
      "predictor.predictor.blocks.1.norm2.bias\n",
      "predictor.predictor.blocks.1.norm2.weight\n",
      "predictor.predictor.blocks.2.attn.proj.bias\n",
      "predictor.predictor.blocks.2.attn.proj.weight\n",
      "predictor.predictor.blocks.2.attn.qkv.bias\n",
      "predictor.predictor.blocks.2.attn.qkv.weight\n",
      "predictor.predictor.blocks.2.mlp.fc1.bias\n",
      "predictor.predictor.blocks.2.mlp.fc1.weight\n",
      "predictor.predictor.blocks.2.mlp.fc2.bias\n",
      "predictor.predictor.blocks.2.mlp.fc2.weight\n",
      "predictor.predictor.blocks.2.norm1.bias\n",
      "predictor.predictor.blocks.2.norm1.weight\n",
      "predictor.predictor.blocks.2.norm2.bias\n",
      "predictor.predictor.blocks.2.norm2.weight\n",
      "predictor.predictor.blocks.3.attn.proj.bias\n",
      "predictor.predictor.blocks.3.attn.proj.weight\n",
      "predictor.predictor.blocks.3.attn.qkv.bias\n",
      "predictor.predictor.blocks.3.attn.qkv.weight\n",
      "predictor.predictor.blocks.3.mlp.fc1.bias\n",
      "predictor.predictor.blocks.3.mlp.fc1.weight\n",
      "predictor.predictor.blocks.3.mlp.fc2.bias\n",
      "predictor.predictor.blocks.3.mlp.fc2.weight\n",
      "predictor.predictor.blocks.3.norm1.bias\n",
      "predictor.predictor.blocks.3.norm1.weight\n",
      "predictor.predictor.blocks.3.norm2.bias\n",
      "predictor.predictor.blocks.3.norm2.weight\n",
      "predictor.predictor.blocks.4.attn.proj.bias\n",
      "predictor.predictor.blocks.4.attn.proj.weight\n",
      "predictor.predictor.blocks.4.attn.qkv.bias\n",
      "predictor.predictor.blocks.4.attn.qkv.weight\n",
      "predictor.predictor.blocks.4.mlp.fc1.bias\n",
      "predictor.predictor.blocks.4.mlp.fc1.weight\n",
      "predictor.predictor.blocks.4.mlp.fc2.bias\n",
      "predictor.predictor.blocks.4.mlp.fc2.weight\n",
      "predictor.predictor.blocks.4.norm1.bias\n",
      "predictor.predictor.blocks.4.norm1.weight\n",
      "predictor.predictor.blocks.4.norm2.bias\n",
      "predictor.predictor.blocks.4.norm2.weight\n",
      "predictor.predictor.blocks.5.attn.proj.bias\n",
      "predictor.predictor.blocks.5.attn.proj.weight\n",
      "predictor.predictor.blocks.5.attn.qkv.bias\n",
      "predictor.predictor.blocks.5.attn.qkv.weight\n",
      "predictor.predictor.blocks.5.mlp.fc1.bias\n",
      "predictor.predictor.blocks.5.mlp.fc1.weight\n",
      "predictor.predictor.blocks.5.mlp.fc2.bias\n",
      "predictor.predictor.blocks.5.mlp.fc2.weight\n",
      "predictor.predictor.blocks.5.norm1.bias\n",
      "predictor.predictor.blocks.5.norm1.weight\n",
      "predictor.predictor.blocks.5.norm2.bias\n",
      "predictor.predictor.blocks.5.norm2.weight\n",
      "predictor.predictor.norm.bias\n",
      "predictor.predictor.norm.weight\n",
      "predictor.predictor_embed.bias\n",
      "predictor.predictor_embed.weight\n",
      "predictor.predictor_norm.bias\n",
      "predictor.predictor_norm.weight\n",
      "predictor.predictor_proj.bias\n",
      "predictor.predictor_proj.weight\n",
      "student.blocks.0.attn.proj.bias\n",
      "student.blocks.0.attn.proj.weight\n",
      "student.blocks.0.attn.qkv.bias\n",
      "student.blocks.0.attn.qkv.weight\n",
      "student.blocks.0.mlp.fc1.bias\n",
      "student.blocks.0.mlp.fc1.weight\n",
      "student.blocks.0.mlp.fc2.bias\n",
      "student.blocks.0.mlp.fc2.weight\n",
      "student.blocks.0.norm1.bias\n",
      "student.blocks.0.norm1.weight\n",
      "student.blocks.0.norm2.bias\n",
      "student.blocks.0.norm2.weight\n",
      "student.blocks.1.attn.proj.bias\n",
      "student.blocks.1.attn.proj.weight\n",
      "student.blocks.1.attn.qkv.bias\n",
      "student.blocks.1.attn.qkv.weight\n",
      "student.blocks.1.mlp.fc1.bias\n",
      "student.blocks.1.mlp.fc1.weight\n",
      "student.blocks.1.mlp.fc2.bias\n",
      "student.blocks.1.mlp.fc2.weight\n",
      "student.blocks.1.norm1.bias\n",
      "student.blocks.1.norm1.weight\n",
      "student.blocks.1.norm2.bias\n",
      "student.blocks.1.norm2.weight\n",
      "student.blocks.10.attn.proj.bias\n",
      "student.blocks.10.attn.proj.weight\n",
      "student.blocks.10.attn.qkv.bias\n",
      "student.blocks.10.attn.qkv.weight\n",
      "student.blocks.10.mlp.fc1.bias\n",
      "student.blocks.10.mlp.fc1.weight\n",
      "student.blocks.10.mlp.fc2.bias\n",
      "student.blocks.10.mlp.fc2.weight\n",
      "student.blocks.10.norm1.bias\n",
      "student.blocks.10.norm1.weight\n",
      "student.blocks.10.norm2.bias\n",
      "student.blocks.10.norm2.weight\n",
      "student.blocks.11.attn.proj.bias\n",
      "student.blocks.11.attn.proj.weight\n",
      "student.blocks.11.attn.qkv.bias\n",
      "student.blocks.11.attn.qkv.weight\n",
      "student.blocks.11.mlp.fc1.bias\n",
      "student.blocks.11.mlp.fc1.weight\n",
      "student.blocks.11.mlp.fc2.bias\n",
      "student.blocks.11.mlp.fc2.weight\n",
      "student.blocks.11.norm1.bias\n",
      "student.blocks.11.norm1.weight\n",
      "student.blocks.11.norm2.bias\n",
      "student.blocks.11.norm2.weight\n",
      "student.blocks.2.attn.proj.bias\n",
      "student.blocks.2.attn.proj.weight\n",
      "student.blocks.2.attn.qkv.bias\n",
      "student.blocks.2.attn.qkv.weight\n",
      "student.blocks.2.mlp.fc1.bias\n",
      "student.blocks.2.mlp.fc1.weight\n",
      "student.blocks.2.mlp.fc2.bias\n",
      "student.blocks.2.mlp.fc2.weight\n",
      "student.blocks.2.norm1.bias\n",
      "student.blocks.2.norm1.weight\n",
      "student.blocks.2.norm2.bias\n",
      "student.blocks.2.norm2.weight\n",
      "student.blocks.3.attn.proj.bias\n",
      "student.blocks.3.attn.proj.weight\n",
      "student.blocks.3.attn.qkv.bias\n",
      "student.blocks.3.attn.qkv.weight\n",
      "student.blocks.3.mlp.fc1.bias\n",
      "student.blocks.3.mlp.fc1.weight\n",
      "student.blocks.3.mlp.fc2.bias\n",
      "student.blocks.3.mlp.fc2.weight\n",
      "student.blocks.3.norm1.bias\n",
      "student.blocks.3.norm1.weight\n",
      "student.blocks.3.norm2.bias\n",
      "student.blocks.3.norm2.weight\n",
      "student.blocks.4.attn.proj.bias\n",
      "student.blocks.4.attn.proj.weight\n",
      "student.blocks.4.attn.qkv.bias\n",
      "student.blocks.4.attn.qkv.weight\n",
      "student.blocks.4.mlp.fc1.bias\n",
      "student.blocks.4.mlp.fc1.weight\n",
      "student.blocks.4.mlp.fc2.bias\n",
      "student.blocks.4.mlp.fc2.weight\n",
      "student.blocks.4.norm1.bias\n",
      "student.blocks.4.norm1.weight\n",
      "student.blocks.4.norm2.bias\n",
      "student.blocks.4.norm2.weight\n",
      "student.blocks.5.attn.proj.bias\n",
      "student.blocks.5.attn.proj.weight\n",
      "student.blocks.5.attn.qkv.bias\n",
      "student.blocks.5.attn.qkv.weight\n",
      "student.blocks.5.mlp.fc1.bias\n",
      "student.blocks.5.mlp.fc1.weight\n",
      "student.blocks.5.mlp.fc2.bias\n",
      "student.blocks.5.mlp.fc2.weight\n",
      "student.blocks.5.norm1.bias\n",
      "student.blocks.5.norm1.weight\n",
      "student.blocks.5.norm2.bias\n",
      "student.blocks.5.norm2.weight\n",
      "student.blocks.6.attn.proj.bias\n",
      "student.blocks.6.attn.proj.weight\n",
      "student.blocks.6.attn.qkv.bias\n",
      "student.blocks.6.attn.qkv.weight\n",
      "student.blocks.6.mlp.fc1.bias\n",
      "student.blocks.6.mlp.fc1.weight\n",
      "student.blocks.6.mlp.fc2.bias\n",
      "student.blocks.6.mlp.fc2.weight\n",
      "student.blocks.6.norm1.bias\n",
      "student.blocks.6.norm1.weight\n",
      "student.blocks.6.norm2.bias\n",
      "student.blocks.6.norm2.weight\n",
      "student.blocks.7.attn.proj.bias\n",
      "student.blocks.7.attn.proj.weight\n",
      "student.blocks.7.attn.qkv.bias\n",
      "student.blocks.7.attn.qkv.weight\n",
      "student.blocks.7.mlp.fc1.bias\n",
      "student.blocks.7.mlp.fc1.weight\n",
      "student.blocks.7.mlp.fc2.bias\n",
      "student.blocks.7.mlp.fc2.weight\n",
      "student.blocks.7.norm1.bias\n",
      "student.blocks.7.norm1.weight\n",
      "student.blocks.7.norm2.bias\n",
      "student.blocks.7.norm2.weight\n",
      "student.blocks.8.attn.proj.bias\n",
      "student.blocks.8.attn.proj.weight\n",
      "student.blocks.8.attn.qkv.bias\n",
      "student.blocks.8.attn.qkv.weight\n",
      "student.blocks.8.mlp.fc1.bias\n",
      "student.blocks.8.mlp.fc1.weight\n",
      "student.blocks.8.mlp.fc2.bias\n",
      "student.blocks.8.mlp.fc2.weight\n",
      "student.blocks.8.norm1.bias\n",
      "student.blocks.8.norm1.weight\n",
      "student.blocks.8.norm2.bias\n",
      "student.blocks.8.norm2.weight\n",
      "student.blocks.9.attn.proj.bias\n",
      "student.blocks.9.attn.proj.weight\n",
      "student.blocks.9.attn.qkv.bias\n",
      "student.blocks.9.attn.qkv.weight\n",
      "student.blocks.9.mlp.fc1.bias\n",
      "student.blocks.9.mlp.fc1.weight\n",
      "student.blocks.9.mlp.fc2.bias\n",
      "student.blocks.9.mlp.fc2.weight\n",
      "student.blocks.9.norm1.bias\n",
      "student.blocks.9.norm1.weight\n",
      "student.blocks.9.norm2.bias\n",
      "student.blocks.9.norm2.weight\n",
      "student.norm.bias\n",
      "student.norm.weight\n",
      "teacher.ema_model.blocks.0.attn.proj.bias\n",
      "teacher.ema_model.blocks.0.attn.proj.weight\n",
      "teacher.ema_model.blocks.0.attn.qkv.bias\n",
      "teacher.ema_model.blocks.0.attn.qkv.weight\n",
      "teacher.ema_model.blocks.0.mlp.fc1.bias\n",
      "teacher.ema_model.blocks.0.mlp.fc1.weight\n",
      "teacher.ema_model.blocks.0.mlp.fc2.bias\n",
      "teacher.ema_model.blocks.0.mlp.fc2.weight\n",
      "teacher.ema_model.blocks.0.norm1.bias\n",
      "teacher.ema_model.blocks.0.norm1.weight\n",
      "teacher.ema_model.blocks.0.norm2.bias\n",
      "teacher.ema_model.blocks.0.norm2.weight\n",
      "teacher.ema_model.blocks.1.attn.proj.bias\n",
      "teacher.ema_model.blocks.1.attn.proj.weight\n",
      "teacher.ema_model.blocks.1.attn.qkv.bias\n",
      "teacher.ema_model.blocks.1.attn.qkv.weight\n",
      "teacher.ema_model.blocks.1.mlp.fc1.bias\n",
      "teacher.ema_model.blocks.1.mlp.fc1.weight\n",
      "teacher.ema_model.blocks.1.mlp.fc2.bias\n",
      "teacher.ema_model.blocks.1.mlp.fc2.weight\n",
      "teacher.ema_model.blocks.1.norm1.bias\n",
      "teacher.ema_model.blocks.1.norm1.weight\n",
      "teacher.ema_model.blocks.1.norm2.bias\n",
      "teacher.ema_model.blocks.1.norm2.weight\n",
      "teacher.ema_model.blocks.10.attn.proj.bias\n",
      "teacher.ema_model.blocks.10.attn.proj.weight\n",
      "teacher.ema_model.blocks.10.attn.qkv.bias\n",
      "teacher.ema_model.blocks.10.attn.qkv.weight\n",
      "teacher.ema_model.blocks.10.mlp.fc1.bias\n",
      "teacher.ema_model.blocks.10.mlp.fc1.weight\n",
      "teacher.ema_model.blocks.10.mlp.fc2.bias\n",
      "teacher.ema_model.blocks.10.mlp.fc2.weight\n",
      "teacher.ema_model.blocks.10.norm1.bias\n",
      "teacher.ema_model.blocks.10.norm1.weight\n",
      "teacher.ema_model.blocks.10.norm2.bias\n",
      "teacher.ema_model.blocks.10.norm2.weight\n",
      "teacher.ema_model.blocks.11.attn.proj.bias\n",
      "teacher.ema_model.blocks.11.attn.proj.weight\n",
      "teacher.ema_model.blocks.11.attn.qkv.bias\n",
      "teacher.ema_model.blocks.11.attn.qkv.weight\n",
      "teacher.ema_model.blocks.11.mlp.fc1.bias\n",
      "teacher.ema_model.blocks.11.mlp.fc1.weight\n",
      "teacher.ema_model.blocks.11.mlp.fc2.bias\n",
      "teacher.ema_model.blocks.11.mlp.fc2.weight\n",
      "teacher.ema_model.blocks.11.norm1.bias\n",
      "teacher.ema_model.blocks.11.norm1.weight\n",
      "teacher.ema_model.blocks.11.norm2.bias\n",
      "teacher.ema_model.blocks.11.norm2.weight\n",
      "teacher.ema_model.blocks.2.attn.proj.bias\n",
      "teacher.ema_model.blocks.2.attn.proj.weight\n",
      "teacher.ema_model.blocks.2.attn.qkv.bias\n",
      "teacher.ema_model.blocks.2.attn.qkv.weight\n",
      "teacher.ema_model.blocks.2.mlp.fc1.bias\n",
      "teacher.ema_model.blocks.2.mlp.fc1.weight\n",
      "teacher.ema_model.blocks.2.mlp.fc2.bias\n",
      "teacher.ema_model.blocks.2.mlp.fc2.weight\n",
      "teacher.ema_model.blocks.2.norm1.bias\n",
      "teacher.ema_model.blocks.2.norm1.weight\n",
      "teacher.ema_model.blocks.2.norm2.bias\n",
      "teacher.ema_model.blocks.2.norm2.weight\n",
      "teacher.ema_model.blocks.3.attn.proj.bias\n",
      "teacher.ema_model.blocks.3.attn.proj.weight\n",
      "teacher.ema_model.blocks.3.attn.qkv.bias\n",
      "teacher.ema_model.blocks.3.attn.qkv.weight\n",
      "teacher.ema_model.blocks.3.mlp.fc1.bias\n",
      "teacher.ema_model.blocks.3.mlp.fc1.weight\n",
      "teacher.ema_model.blocks.3.mlp.fc2.bias\n",
      "teacher.ema_model.blocks.3.mlp.fc2.weight\n",
      "teacher.ema_model.blocks.3.norm1.bias\n",
      "teacher.ema_model.blocks.3.norm1.weight\n",
      "teacher.ema_model.blocks.3.norm2.bias\n",
      "teacher.ema_model.blocks.3.norm2.weight\n",
      "teacher.ema_model.blocks.4.attn.proj.bias\n",
      "teacher.ema_model.blocks.4.attn.proj.weight\n",
      "teacher.ema_model.blocks.4.attn.qkv.bias\n",
      "teacher.ema_model.blocks.4.attn.qkv.weight\n",
      "teacher.ema_model.blocks.4.mlp.fc1.bias\n",
      "teacher.ema_model.blocks.4.mlp.fc1.weight\n",
      "teacher.ema_model.blocks.4.mlp.fc2.bias\n",
      "teacher.ema_model.blocks.4.mlp.fc2.weight\n",
      "teacher.ema_model.blocks.4.norm1.bias\n",
      "teacher.ema_model.blocks.4.norm1.weight\n",
      "teacher.ema_model.blocks.4.norm2.bias\n",
      "teacher.ema_model.blocks.4.norm2.weight\n",
      "teacher.ema_model.blocks.5.attn.proj.bias\n",
      "teacher.ema_model.blocks.5.attn.proj.weight\n",
      "teacher.ema_model.blocks.5.attn.qkv.bias\n",
      "teacher.ema_model.blocks.5.attn.qkv.weight\n",
      "teacher.ema_model.blocks.5.mlp.fc1.bias\n",
      "teacher.ema_model.blocks.5.mlp.fc1.weight\n",
      "teacher.ema_model.blocks.5.mlp.fc2.bias\n",
      "teacher.ema_model.blocks.5.mlp.fc2.weight\n",
      "teacher.ema_model.blocks.5.norm1.bias\n",
      "teacher.ema_model.blocks.5.norm1.weight\n",
      "teacher.ema_model.blocks.5.norm2.bias\n",
      "teacher.ema_model.blocks.5.norm2.weight\n",
      "teacher.ema_model.blocks.6.attn.proj.bias\n",
      "teacher.ema_model.blocks.6.attn.proj.weight\n",
      "teacher.ema_model.blocks.6.attn.qkv.bias\n",
      "teacher.ema_model.blocks.6.attn.qkv.weight\n",
      "teacher.ema_model.blocks.6.mlp.fc1.bias\n",
      "teacher.ema_model.blocks.6.mlp.fc1.weight\n",
      "teacher.ema_model.blocks.6.mlp.fc2.bias\n",
      "teacher.ema_model.blocks.6.mlp.fc2.weight\n",
      "teacher.ema_model.blocks.6.norm1.bias\n",
      "teacher.ema_model.blocks.6.norm1.weight\n",
      "teacher.ema_model.blocks.6.norm2.bias\n",
      "teacher.ema_model.blocks.6.norm2.weight\n",
      "teacher.ema_model.blocks.7.attn.proj.bias\n",
      "teacher.ema_model.blocks.7.attn.proj.weight\n",
      "teacher.ema_model.blocks.7.attn.qkv.bias\n",
      "teacher.ema_model.blocks.7.attn.qkv.weight\n",
      "teacher.ema_model.blocks.7.mlp.fc1.bias\n",
      "teacher.ema_model.blocks.7.mlp.fc1.weight\n",
      "teacher.ema_model.blocks.7.mlp.fc2.bias\n",
      "teacher.ema_model.blocks.7.mlp.fc2.weight\n",
      "teacher.ema_model.blocks.7.norm1.bias\n",
      "teacher.ema_model.blocks.7.norm1.weight\n",
      "teacher.ema_model.blocks.7.norm2.bias\n",
      "teacher.ema_model.blocks.7.norm2.weight\n",
      "teacher.ema_model.blocks.8.attn.proj.bias\n",
      "teacher.ema_model.blocks.8.attn.proj.weight\n",
      "teacher.ema_model.blocks.8.attn.qkv.bias\n",
      "teacher.ema_model.blocks.8.attn.qkv.weight\n",
      "teacher.ema_model.blocks.8.mlp.fc1.bias\n",
      "teacher.ema_model.blocks.8.mlp.fc1.weight\n",
      "teacher.ema_model.blocks.8.mlp.fc2.bias\n",
      "teacher.ema_model.blocks.8.mlp.fc2.weight\n",
      "teacher.ema_model.blocks.8.norm1.bias\n",
      "teacher.ema_model.blocks.8.norm1.weight\n",
      "teacher.ema_model.blocks.8.norm2.bias\n",
      "teacher.ema_model.blocks.8.norm2.weight\n",
      "teacher.ema_model.blocks.9.attn.proj.bias\n",
      "teacher.ema_model.blocks.9.attn.proj.weight\n",
      "teacher.ema_model.blocks.9.attn.qkv.bias\n",
      "teacher.ema_model.blocks.9.attn.qkv.weight\n",
      "teacher.ema_model.blocks.9.mlp.fc1.bias\n",
      "teacher.ema_model.blocks.9.mlp.fc1.weight\n",
      "teacher.ema_model.blocks.9.mlp.fc2.bias\n",
      "teacher.ema_model.blocks.9.mlp.fc2.weight\n",
      "teacher.ema_model.blocks.9.norm1.bias\n",
      "teacher.ema_model.blocks.9.norm1.weight\n",
      "teacher.ema_model.blocks.9.norm2.bias\n",
      "teacher.ema_model.blocks.9.norm2.weight\n",
      "teacher.ema_model.norm.bias\n",
      "teacher.ema_model.norm.weight\n",
      "teacher.initted\n",
      "teacher.online_model.blocks.0.attn.proj.bias\n",
      "teacher.online_model.blocks.0.attn.proj.weight\n",
      "teacher.online_model.blocks.0.attn.qkv.bias\n",
      "teacher.online_model.blocks.0.attn.qkv.weight\n",
      "teacher.online_model.blocks.0.mlp.fc1.bias\n",
      "teacher.online_model.blocks.0.mlp.fc1.weight\n",
      "teacher.online_model.blocks.0.mlp.fc2.bias\n",
      "teacher.online_model.blocks.0.mlp.fc2.weight\n",
      "teacher.online_model.blocks.0.norm1.bias\n",
      "teacher.online_model.blocks.0.norm1.weight\n",
      "teacher.online_model.blocks.0.norm2.bias\n",
      "teacher.online_model.blocks.0.norm2.weight\n",
      "teacher.online_model.blocks.1.attn.proj.bias\n",
      "teacher.online_model.blocks.1.attn.proj.weight\n",
      "teacher.online_model.blocks.1.attn.qkv.bias\n",
      "teacher.online_model.blocks.1.attn.qkv.weight\n",
      "teacher.online_model.blocks.1.mlp.fc1.bias\n",
      "teacher.online_model.blocks.1.mlp.fc1.weight\n",
      "teacher.online_model.blocks.1.mlp.fc2.bias\n",
      "teacher.online_model.blocks.1.mlp.fc2.weight\n",
      "teacher.online_model.blocks.1.norm1.bias\n",
      "teacher.online_model.blocks.1.norm1.weight\n",
      "teacher.online_model.blocks.1.norm2.bias\n",
      "teacher.online_model.blocks.1.norm2.weight\n",
      "teacher.online_model.blocks.10.attn.proj.bias\n",
      "teacher.online_model.blocks.10.attn.proj.weight\n",
      "teacher.online_model.blocks.10.attn.qkv.bias\n",
      "teacher.online_model.blocks.10.attn.qkv.weight\n",
      "teacher.online_model.blocks.10.mlp.fc1.bias\n",
      "teacher.online_model.blocks.10.mlp.fc1.weight\n",
      "teacher.online_model.blocks.10.mlp.fc2.bias\n",
      "teacher.online_model.blocks.10.mlp.fc2.weight\n",
      "teacher.online_model.blocks.10.norm1.bias\n",
      "teacher.online_model.blocks.10.norm1.weight\n",
      "teacher.online_model.blocks.10.norm2.bias\n",
      "teacher.online_model.blocks.10.norm2.weight\n",
      "teacher.online_model.blocks.11.attn.proj.bias\n",
      "teacher.online_model.blocks.11.attn.proj.weight\n",
      "teacher.online_model.blocks.11.attn.qkv.bias\n",
      "teacher.online_model.blocks.11.attn.qkv.weight\n",
      "teacher.online_model.blocks.11.mlp.fc1.bias\n",
      "teacher.online_model.blocks.11.mlp.fc1.weight\n",
      "teacher.online_model.blocks.11.mlp.fc2.bias\n",
      "teacher.online_model.blocks.11.mlp.fc2.weight\n",
      "teacher.online_model.blocks.11.norm1.bias\n",
      "teacher.online_model.blocks.11.norm1.weight\n",
      "teacher.online_model.blocks.11.norm2.bias\n",
      "teacher.online_model.blocks.11.norm2.weight\n",
      "teacher.online_model.blocks.2.attn.proj.bias\n",
      "teacher.online_model.blocks.2.attn.proj.weight\n",
      "teacher.online_model.blocks.2.attn.qkv.bias\n",
      "teacher.online_model.blocks.2.attn.qkv.weight\n",
      "teacher.online_model.blocks.2.mlp.fc1.bias\n",
      "teacher.online_model.blocks.2.mlp.fc1.weight\n",
      "teacher.online_model.blocks.2.mlp.fc2.bias\n",
      "teacher.online_model.blocks.2.mlp.fc2.weight\n",
      "teacher.online_model.blocks.2.norm1.bias\n",
      "teacher.online_model.blocks.2.norm1.weight\n",
      "teacher.online_model.blocks.2.norm2.bias\n",
      "teacher.online_model.blocks.2.norm2.weight\n",
      "teacher.online_model.blocks.3.attn.proj.bias\n",
      "teacher.online_model.blocks.3.attn.proj.weight\n",
      "teacher.online_model.blocks.3.attn.qkv.bias\n",
      "teacher.online_model.blocks.3.attn.qkv.weight\n",
      "teacher.online_model.blocks.3.mlp.fc1.bias\n",
      "teacher.online_model.blocks.3.mlp.fc1.weight\n",
      "teacher.online_model.blocks.3.mlp.fc2.bias\n",
      "teacher.online_model.blocks.3.mlp.fc2.weight\n",
      "teacher.online_model.blocks.3.norm1.bias\n",
      "teacher.online_model.blocks.3.norm1.weight\n",
      "teacher.online_model.blocks.3.norm2.bias\n",
      "teacher.online_model.blocks.3.norm2.weight\n",
      "teacher.online_model.blocks.4.attn.proj.bias\n",
      "teacher.online_model.blocks.4.attn.proj.weight\n",
      "teacher.online_model.blocks.4.attn.qkv.bias\n",
      "teacher.online_model.blocks.4.attn.qkv.weight\n",
      "teacher.online_model.blocks.4.mlp.fc1.bias\n",
      "teacher.online_model.blocks.4.mlp.fc1.weight\n",
      "teacher.online_model.blocks.4.mlp.fc2.bias\n",
      "teacher.online_model.blocks.4.mlp.fc2.weight\n",
      "teacher.online_model.blocks.4.norm1.bias\n",
      "teacher.online_model.blocks.4.norm1.weight\n",
      "teacher.online_model.blocks.4.norm2.bias\n",
      "teacher.online_model.blocks.4.norm2.weight\n",
      "teacher.online_model.blocks.5.attn.proj.bias\n",
      "teacher.online_model.blocks.5.attn.proj.weight\n",
      "teacher.online_model.blocks.5.attn.qkv.bias\n",
      "teacher.online_model.blocks.5.attn.qkv.weight\n",
      "teacher.online_model.blocks.5.mlp.fc1.bias\n",
      "teacher.online_model.blocks.5.mlp.fc1.weight\n",
      "teacher.online_model.blocks.5.mlp.fc2.bias\n",
      "teacher.online_model.blocks.5.mlp.fc2.weight\n",
      "teacher.online_model.blocks.5.norm1.bias\n",
      "teacher.online_model.blocks.5.norm1.weight\n",
      "teacher.online_model.blocks.5.norm2.bias\n",
      "teacher.online_model.blocks.5.norm2.weight\n",
      "teacher.online_model.blocks.6.attn.proj.bias\n",
      "teacher.online_model.blocks.6.attn.proj.weight\n",
      "teacher.online_model.blocks.6.attn.qkv.bias\n",
      "teacher.online_model.blocks.6.attn.qkv.weight\n",
      "teacher.online_model.blocks.6.mlp.fc1.bias\n",
      "teacher.online_model.blocks.6.mlp.fc1.weight\n",
      "teacher.online_model.blocks.6.mlp.fc2.bias\n",
      "teacher.online_model.blocks.6.mlp.fc2.weight\n",
      "teacher.online_model.blocks.6.norm1.bias\n",
      "teacher.online_model.blocks.6.norm1.weight\n",
      "teacher.online_model.blocks.6.norm2.bias\n",
      "teacher.online_model.blocks.6.norm2.weight\n",
      "teacher.online_model.blocks.7.attn.proj.bias\n",
      "teacher.online_model.blocks.7.attn.proj.weight\n",
      "teacher.online_model.blocks.7.attn.qkv.bias\n",
      "teacher.online_model.blocks.7.attn.qkv.weight\n",
      "teacher.online_model.blocks.7.mlp.fc1.bias\n",
      "teacher.online_model.blocks.7.mlp.fc1.weight\n",
      "teacher.online_model.blocks.7.mlp.fc2.bias\n",
      "teacher.online_model.blocks.7.mlp.fc2.weight\n",
      "teacher.online_model.blocks.7.norm1.bias\n",
      "teacher.online_model.blocks.7.norm1.weight\n",
      "teacher.online_model.blocks.7.norm2.bias\n",
      "teacher.online_model.blocks.7.norm2.weight\n",
      "teacher.online_model.blocks.8.attn.proj.bias\n",
      "teacher.online_model.blocks.8.attn.proj.weight\n",
      "teacher.online_model.blocks.8.attn.qkv.bias\n",
      "teacher.online_model.blocks.8.attn.qkv.weight\n",
      "teacher.online_model.blocks.8.mlp.fc1.bias\n",
      "teacher.online_model.blocks.8.mlp.fc1.weight\n",
      "teacher.online_model.blocks.8.mlp.fc2.bias\n",
      "teacher.online_model.blocks.8.mlp.fc2.weight\n",
      "teacher.online_model.blocks.8.norm1.bias\n",
      "teacher.online_model.blocks.8.norm1.weight\n",
      "teacher.online_model.blocks.8.norm2.bias\n",
      "teacher.online_model.blocks.8.norm2.weight\n",
      "teacher.online_model.blocks.9.attn.proj.bias\n",
      "teacher.online_model.blocks.9.attn.proj.weight\n",
      "teacher.online_model.blocks.9.attn.qkv.bias\n",
      "teacher.online_model.blocks.9.attn.qkv.weight\n",
      "teacher.online_model.blocks.9.mlp.fc1.bias\n",
      "teacher.online_model.blocks.9.mlp.fc1.weight\n",
      "teacher.online_model.blocks.9.mlp.fc2.bias\n",
      "teacher.online_model.blocks.9.mlp.fc2.weight\n",
      "teacher.online_model.blocks.9.norm1.bias\n",
      "teacher.online_model.blocks.9.norm1.weight\n",
      "teacher.online_model.blocks.9.norm2.bias\n",
      "teacher.online_model.blocks.9.norm2.weight\n",
      "teacher.online_model.norm.bias\n",
      "teacher.online_model.norm.weight\n",
      "teacher.step\n",
      "tokenizer.embedding.encoder.first_conv.0.weight\n",
      "tokenizer.embedding.encoder.first_conv.1.bias\n",
      "tokenizer.embedding.encoder.first_conv.1.num_batches_tracked\n",
      "tokenizer.embedding.encoder.first_conv.1.running_mean\n",
      "tokenizer.embedding.encoder.first_conv.1.running_var\n",
      "tokenizer.embedding.encoder.first_conv.1.weight\n",
      "tokenizer.embedding.encoder.first_conv.3.bias\n",
      "tokenizer.embedding.encoder.first_conv.3.weight\n",
      "tokenizer.embedding.encoder.second_conv.0.weight\n",
      "tokenizer.embedding.encoder.second_conv.1.bias\n",
      "tokenizer.embedding.encoder.second_conv.1.num_batches_tracked\n",
      "tokenizer.embedding.encoder.second_conv.1.running_mean\n",
      "tokenizer.embedding.encoder.second_conv.1.running_var\n",
      "tokenizer.embedding.encoder.second_conv.1.weight\n",
      "tokenizer.embedding.encoder.second_conv.3.bias\n",
      "tokenizer.embedding.encoder.second_conv.3.weight\n"
     ]
    }
   ],
   "source": [
    "inspect_checkpoint(\"../configs/checkpoints/jepa_no_FT.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae74671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
