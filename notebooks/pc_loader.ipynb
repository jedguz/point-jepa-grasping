{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f191891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ShapeNet .npz Model Downloader from Google Cloud Storage\n",
    "\n",
    "This script downloads specific .npz ShapeNet models from a Google Cloud Storage bucket.\n",
    "\n",
    "Prerequisites:\n",
    "- Google Cloud SDK installed and configured\n",
    "- Appropriate permissions to access the GCS bucket\n",
    "- Required Python packages: google-cloud-storage, tqdm, numpy\n",
    "\n",
    "Usage:\n",
    "    python shapenet_downloader.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c083301",
   "metadata": {},
   "source": [
    "# ShapeNet .npz Model Downloader from Google Cloud Storage\n",
    "\n",
    "This script downloads specific .npz ShapeNet models from a Google Cloud Storage bucket.\n",
    "\n",
    "One can choose shapenet categories and predetermined resolutions of the point clouds to extract in:\n",
    "CATEGORIES_TO_EXTRACT and RESOLUTION_TO_EXTRACT.\n",
    "\n",
    "## Prerequisites\n",
    "- Google Cloud SDK installed and configured\n",
    "- Appropriate permissions to access the GCS bucket\n",
    "- Required Python packages installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce73d3",
   "metadata": {
    "title": "Import Libraries"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da85b606",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Configuration"
   },
   "outputs": [],
   "source": [
    "# Configuration settings\n",
    "BUCKET_NAME = \"adlr2025-pointclouds\"  # Replace with your bucket name\n",
    "PROJECT_ID = \"adlr-2025\"  # Replace with your GCP project ID\n",
    "LOCAL_DOWNLOAD_PATH = \"../src/data/shapenet_extracted\"  # Local directory to save models\n",
    "\n",
    "# Create local directory if it doesn't exist\n",
    "Path(LOCAL_DOWNLOAD_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Local download path: {LOCAL_DOWNLOAD_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category filtering - Set to None to extract all, or specify categories to extract\n",
    "# CATEGORIES_TO_EXTRACT = None             # Extract all categories\n",
    "CATEGORIES_TO_EXTRACT = [\"02946921\", \"02880940\", \"03085013\"]  # Only airplane, car, chair\n",
    "\n",
    "if CATEGORIES_TO_EXTRACT:\n",
    "    print(f\"Categories to extract: {len(CATEGORIES_TO_EXTRACT)} categories\")\n",
    "    for cat in CATEGORIES_TO_EXTRACT:\n",
    "        print(f\"  - {cat}\")\n",
    "else:\n",
    "    print(\"Extracting all categories\")\n",
    "\n",
    "# Resolution filtering - Set to None to extract all resolutions, or specify resolutions to extract\n",
    "# RESOLUTION_TO_EXTRACT = None             # Extract all resolutions\n",
    "RESOLUTION_TO_EXTRACT = [2048]       # Only specific resolutions\n",
    "\n",
    "if RESOLUTION_TO_EXTRACT:\n",
    "    print(f\"Resolutions to extract: {RESOLUTION_TO_EXTRACT}\")\n",
    "else:\n",
    "    print(\"Extracting all resolutions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b98a6f",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Authentication and Client Setup"
   },
   "outputs": [],
   "source": [
    "def setup_gcs_client():\n",
    "    \"\"\"Initialize the Google Cloud Storage client\"\"\"\n",
    "    try:\n",
    "        client = storage.Client(project=PROJECT_ID)\n",
    "        bucket = client.bucket(BUCKET_NAME)\n",
    "        print(\"‚úì Successfully connected to Google Cloud Storage\")\n",
    "        return client, bucket\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error connecting to GCS: {e}\")\n",
    "        print(\"Make sure you have authenticated with 'gcloud auth login' or set up service account credentials\")\n",
    "        return None, None\n",
    "\n",
    "# Initialize client and bucket\n",
    "client, bucket = setup_gcs_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dacf0f3",
   "metadata": {
    "title": "List Available .npz Files"
   },
   "outputs": [],
   "source": [
    "def list_npz_files_in_categories(bucket, categories_to_extract=None, max_files_per_category=50):\n",
    "    \"\"\"\n",
    "    List all .npz files in specified category folders\n",
    "    \n",
    "    Args:\n",
    "        bucket: GCS bucket object\n",
    "        categories_to_extract: List of category folder names, or None for all\n",
    "        max_files_per_category: Maximum number of files to return per category\n",
    "    \n",
    "    Returns:\n",
    "        List of file information dictionaries\n",
    "    \"\"\"\n",
    "    npz_files = []\n",
    "    \n",
    "    if categories_to_extract is None:\n",
    "        # If no categories specified, scan entire bucket\n",
    "        print(\"Scanning entire bucket for .npz files...\")\n",
    "        blobs = bucket.list_blobs()\n",
    "        \n",
    "        for blob in blobs:\n",
    "            if blob.name.endswith('.npz'):\n",
    "                npz_files.append({\n",
    "                    'name': blob.name,\n",
    "                    'category': blob.name.split('/')[0] if '/' in blob.name else 'root',\n",
    "                    'size_mb': round(blob.size / (1024 * 1024), 2),\n",
    "                    'updated': blob.updated.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "                \n",
    "                if len(npz_files) >= max_files_per_category * 20:  # Reasonable limit for all categories\n",
    "                    break\n",
    "    else:\n",
    "        # Scan specific category folders\n",
    "        print(f\"Scanning {len(categories_to_extract)} category folders for .npz files...\")\n",
    "        \n",
    "        for category in categories_to_extract:\n",
    "            print(f\"  Scanning category: {category}\")\n",
    "            category_files = 0\n",
    "            \n",
    "            # List all blobs in this category folder\n",
    "            blobs = bucket.list_blobs(prefix=f\"{category}/\")\n",
    "            \n",
    "            for blob in blobs:\n",
    "                if blob.name.endswith('.npz'):\n",
    "                    npz_files.append({\n",
    "                        'name': blob.name,\n",
    "                        'category': category,\n",
    "                        'size_mb': round(blob.size / (1024 * 1024), 2),\n",
    "                        'updated': blob.updated.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    })\n",
    "                    category_files += 1\n",
    "                    \n",
    "                    if category_files >= max_files_per_category:\n",
    "                        print(f\"    Found {category_files} files (limit reached)\")\n",
    "                        break\n",
    "            \n",
    "            if category_files < max_files_per_category:\n",
    "                print(f\"    Found {category_files} files\")\n",
    "    \n",
    "    return npz_files\n",
    "\n",
    "# List available .npz files based on category filtering\n",
    "if bucket:\n",
    "    available_files = list_npz_files_in_categories(bucket, CATEGORIES_TO_EXTRACT, max_files_per_category=1000)\n",
    "    \n",
    "    print(f\"\\nFound {len(available_files)} .npz files:\")\n",
    "    \n",
    "    # Group by category for summary\n",
    "    category_counts = {}\n",
    "    for file_info in available_files:\n",
    "        category = file_info['category']\n",
    "        category_counts[category] = category_counts.get(category, 0) + 1\n",
    "    \n",
    "    print(\"\\nFiles by category:\")\n",
    "    for category, count in category_counts.items():\n",
    "        print(f\"  {category}: {count} files\")\n",
    "    \n",
    "    # Show first 10 files\n",
    "    print(f\"\\nFirst 10 files:\")\n",
    "    for i, file_info in enumerate(available_files[:10]):\n",
    "        print(f\"{i+1:2d}. {file_info['name']} ({file_info['size_mb']} MB) [{file_info['category']}]\")\n",
    "    \n",
    "    if len(available_files) > 10:\n",
    "        print(f\"... and {len(available_files) - 10} more files\")\n",
    "else:\n",
    "    available_files = []\n",
    "    print(\"Cannot list files - GCS client not initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf54e45",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Filter Files for Download"
   },
   "outputs": [],
   "source": [
    "# %% Filter Files for Download\n",
    "def filter_files_by_resolution(available_files, resolution_to_extract=None):\n",
    "    \"\"\"\n",
    "    Filter files by resolution based on filename patterns like pc_2048.npz, pc_10240.npz\n",
    "    \n",
    "    Args:\n",
    "        available_files: List of available file information\n",
    "        resolution_to_extract: List of resolutions to extract, or None for all\n",
    "    \n",
    "    Returns:\n",
    "        List of files to download\n",
    "    \"\"\"\n",
    "    files_to_download = []\n",
    "    \n",
    "    for file_info in available_files:\n",
    "        file_name = os.path.basename(file_info['name'])  # Get just the filename\n",
    "        should_download = False\n",
    "        \n",
    "        # If no resolution filter, download all files\n",
    "        if resolution_to_extract is None:\n",
    "            should_download = True\n",
    "            file_info['reason'] = 'all_resolutions'\n",
    "            file_info['resolution'] = 'unknown'\n",
    "        else:\n",
    "            # Check if filename matches pattern pc_XXXX.npz\n",
    "            for resolution in resolution_to_extract:\n",
    "                if file_name == f\"pc_{resolution}.npz\":\n",
    "                    should_download = True\n",
    "                    file_info['reason'] = 'resolution_match'\n",
    "                    file_info['resolution'] = resolution\n",
    "                    break\n",
    "            \n",
    "            # If no resolution matched, mark for skipping\n",
    "            if not should_download:\n",
    "                file_info['reason'] = 'resolution_filtered'\n",
    "                # Try to extract resolution from filename for reporting\n",
    "                if file_name.startswith('pc_') and file_name.endswith('.npz'):\n",
    "                    try:\n",
    "                        extracted_res = file_name[3:-4]  # Remove 'pc_' and '.npz'\n",
    "                        file_info['resolution'] = int(extracted_res)\n",
    "                    except ValueError:\n",
    "                        file_info['resolution'] = 'unknown'\n",
    "                else:\n",
    "                    file_info['resolution'] = 'unknown'\n",
    "        \n",
    "        if should_download:\n",
    "            files_to_download.append(file_info)\n",
    "    \n",
    "    return files_to_download\n",
    "\n",
    "# Filter files by resolution\n",
    "files_to_download = filter_files_by_resolution(available_files, RESOLUTION_TO_EXTRACT)\n",
    "\n",
    "print(f\"\\nSelected {len(files_to_download)} files for download:\")\n",
    "total_size_mb = 0\n",
    "for file_info in files_to_download:\n",
    "    total_size_mb += file_info['size_mb']\n",
    "    print(f\"- {file_info['name']} ({file_info['size_mb']} MB) [{file_info['reason']}]\")\n",
    "\n",
    "print(f\"\\nTotal download size: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e080d736",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Download Functions"
   },
   "outputs": [],
   "source": [
    "def download_file(bucket, blob_name, local_path, overwrite=False):\n",
    "    \"\"\"\n",
    "    Download a single file from GCS bucket, preserving folder structure\n",
    "    \n",
    "    Args:\n",
    "        bucket: GCS bucket object\n",
    "        blob_name: Full path of the blob to download (e.g., \"category/model.npz\")\n",
    "        local_path: Local directory path\n",
    "        overwrite: Whether to overwrite existing files\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (success: bool, local_file_path: str)\n",
    "    \"\"\"\n",
    "    # Create local file path preserving the folder structure\n",
    "    local_file_path = os.path.join(local_path, blob_name)\n",
    "    \n",
    "    # Create subdirectories if they don't exist\n",
    "    local_dir = os.path.dirname(local_file_path)\n",
    "    Path(local_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if file already exists\n",
    "    if os.path.exists(local_file_path) and not overwrite:\n",
    "        print(f\"‚è≠Ô∏è  Skipping {blob_name} (already exists)\")\n",
    "        return True, local_file_path\n",
    "    \n",
    "    try:\n",
    "        blob = bucket.blob(blob_name)\n",
    "        blob.download_to_filename(local_file_path)\n",
    "        return True, local_file_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading {blob_name}: {e}\")\n",
    "        return False, None\n",
    "\n",
    "def download_files_batch(bucket, files_to_download, local_path, overwrite=False):\n",
    "    \"\"\"\n",
    "    Download multiple files with progress tracking\n",
    "    \n",
    "    Args:\n",
    "        bucket: GCS bucket object\n",
    "        files_to_download: List of file information dictionaries\n",
    "        local_path: Local directory path\n",
    "        overwrite: Whether to overwrite existing files\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (successful_downloads: list, failed_downloads: list)\n",
    "    \"\"\"\n",
    "    successful_downloads = []\n",
    "    failed_downloads = []\n",
    "    \n",
    "    print(f\"\\nStarting download of {len(files_to_download)} files...\")\n",
    "    print(f\"Download directory: {local_path}\")\n",
    "    \n",
    "    for file_info in tqdm(files_to_download, desc=\"Downloading\"):\n",
    "        blob_name = file_info['name']\n",
    "        success, local_file_path = download_file(bucket, blob_name, local_path, overwrite)\n",
    "        \n",
    "        if success:\n",
    "            successful_downloads.append({\n",
    "                'blob_name': blob_name,\n",
    "                'local_path': local_file_path,\n",
    "                'size_mb': file_info['size_mb']\n",
    "            })\n",
    "            print(f\"‚úÖ Downloaded: {blob_name}\")\n",
    "        else:\n",
    "            failed_downloads.append(blob_name)\n",
    "    \n",
    "    return successful_downloads, failed_downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf25ac3",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Execute Download"
   },
   "outputs": [],
   "source": [
    "def execute_download():\n",
    "    \"\"\"Execute the download process\"\"\"\n",
    "    if not bucket:\n",
    "        print(\"Cannot download - GCS client not initialized\")\n",
    "        return [], []\n",
    "    \n",
    "    if not files_to_download:\n",
    "        print(\"No files selected for download.\")\n",
    "        return [], []\n",
    "    \n",
    "    successful_downloads, failed_downloads = download_files_batch(\n",
    "        bucket, \n",
    "        files_to_download, \n",
    "        LOCAL_DOWNLOAD_PATH, \n",
    "        overwrite=False  # Set to True to overwrite existing files\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"DOWNLOAD SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"‚úÖ Successful downloads: {len(successful_downloads)}\")\n",
    "    print(f\"‚ùå Failed downloads: {len(failed_downloads)}\")\n",
    "    \n",
    "    if successful_downloads:\n",
    "        total_downloaded_mb = sum(d['size_mb'] for d in successful_downloads)\n",
    "        print(f\"üìÅ Total downloaded: {total_downloaded_mb:.2f} MB\")\n",
    "        print(f\"üìÇ Download location: {LOCAL_DOWNLOAD_PATH}\")\n",
    "    \n",
    "    if failed_downloads:\n",
    "        print(f\"\\nFailed downloads:\")\n",
    "        for failed_file in failed_downloads:\n",
    "            print(f\"  - {failed_file}\")\n",
    "    \n",
    "    return successful_downloads, failed_downloads\n",
    "\n",
    "# Execute the download\n",
    "successful_downloads, failed_downloads = execute_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12d5d51",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Verify Downloaded Files"
   },
   "outputs": [],
   "source": [
    "def verify_npz_files(download_path):\n",
    "    \"\"\"\n",
    "    Verify that downloaded .npz files can be loaded\n",
    "    \n",
    "    Args:\n",
    "        download_path: Path to directory containing .npz files\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (verified_files: list, corrupted_files: list)\n",
    "    \"\"\"\n",
    "    npz_files = list(Path(download_path).glob('*.npz'))\n",
    "    \n",
    "    print(f\"\\nVerifying {len(npz_files)} downloaded .npz files...\")\n",
    "    \n",
    "    verified_files = []\n",
    "    corrupted_files = []\n",
    "    \n",
    "    for npz_file in npz_files:\n",
    "        try:\n",
    "            # Try to load the .npz file\n",
    "            with np.load(npz_file) as data:\n",
    "                keys = list(data.keys())\n",
    "                file_info = {\n",
    "                    'filename': npz_file.name,\n",
    "                    'size_mb': round(npz_file.stat().st_size / (1024 * 1024), 2),\n",
    "                    'keys': keys,\n",
    "                    'num_arrays': len(keys)\n",
    "                }\n",
    "                verified_files.append(file_info)\n",
    "                print(f\"‚úÖ {npz_file.name}: {len(keys)} arrays - {keys[:3]}{'...' if len(keys) > 3 else ''}\")\n",
    "        except Exception as e:\n",
    "            corrupted_files.append(npz_file.name)\n",
    "            print(f\"‚ùå {npz_file.name}: Error loading - {e}\")\n",
    "    \n",
    "    print(f\"\\nVerification complete:\")\n",
    "    print(f\"‚úÖ Valid files: {len(verified_files)}\")\n",
    "    print(f\"‚ùå Corrupted files: {len(corrupted_files)}\")\n",
    "    \n",
    "    return verified_files, corrupted_files\n",
    "\n",
    "# Verify downloaded files\n",
    "if os.path.exists(LOCAL_DOWNLOAD_PATH):\n",
    "    verified_files, corrupted_files = verify_npz_files(LOCAL_DOWNLOAD_PATH)\n",
    "else:\n",
    "    verified_files, corrupted_files = [], []\n",
    "    print(f\"Download path {LOCAL_DOWNLOAD_PATH} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c9bfa7",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Sample Data Inspection"
   },
   "outputs": [],
   "source": [
    "def inspect_sample_npz(file_path, max_arrays=5):\n",
    "    \"\"\"\n",
    "    Inspect the contents of a sample .npz file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .npz file\n",
    "        max_arrays: Maximum number of arrays to inspect\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with np.load(file_path) as data:\n",
    "            print(f\"\\nInspecting: {os.path.basename(file_path)}\")\n",
    "            print(f\"File size: {os.path.getsize(file_path) / (1024*1024):.2f} MB\")\n",
    "            print(f\"Number of arrays: {len(data.keys())}\")\n",
    "            print(\"\\nArray details:\")\n",
    "            \n",
    "            for i, key in enumerate(list(data.keys())[:max_arrays]):\n",
    "                arr = data[key]\n",
    "                print(f\"  {key}: shape={arr.shape}, dtype={arr.dtype}, size={arr.nbytes/(1024*1024):.2f}MB\")\n",
    "                \n",
    "                # Show some sample values for small arrays\n",
    "                if arr.size < 20:\n",
    "                    print(f\"    Sample values: {arr.flatten()[:10]}\")\n",
    "            \n",
    "            if len(data.keys()) > max_arrays:\n",
    "                print(f\"  ... and {len(data.keys()) - max_arrays} more arrays\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error inspecting {file_path}: {e}\")\n",
    "\n",
    "# Inspect a sample file if any were downloaded\n",
    "if verified_files:\n",
    "    sample_file = os.path.join(LOCAL_DOWNLOAD_PATH, verified_files[0]['filename'])\n",
    "    inspect_sample_npz(sample_file)\n",
    "else:\n",
    "    print(\"No verified files available for inspection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd68bc8",
   "metadata": {
    "title": "Main Execution"
   },
   "outputs": [],
   "source": [
    "\"\"\"Main function to run the entire download process\"\"\"\n",
    "print(\"ShapeNet NPZ Downloader\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configuration summary\n",
    "print(f\"Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Download path: {LOCAL_DOWNLOAD_PATH}\")\n",
    "print(f\"Files to download: {len(files_to_download)}\")\n",
    "\n",
    "if files_to_download:\n",
    "    # Confirm download\n",
    "    response = input(f\"\\nProceed with downloading {len(files_to_download)} files? (y/n): \")\n",
    "    if response.lower() == 'y':\n",
    "        successful, failed = execute_download()\n",
    "        \n",
    "        if successful:\n",
    "            verify_npz_files(LOCAL_DOWNLOAD_PATH)\n",
    "            print(f\"\\n‚úÖ Download completed successfully!\")\n",
    "            print(f\"üìÅ {len(successful)} files downloaded to {LOCAL_DOWNLOAD_PATH}\")\n",
    "        else:\n",
    "            print(\"‚ùå No files were downloaded successfully.\")\n",
    "    else:\n",
    "        print(\"Download cancelled.\")\n",
    "else:\n",
    "    print(\"No files selected for download.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
