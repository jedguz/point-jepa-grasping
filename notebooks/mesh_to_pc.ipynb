{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff4a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ShapeNet Mesh to Point Cloud Converter\n",
    "======================================\n",
    "\n",
    "This script converts ShapeNet meshes (OBJ files) to point clouds using various sampling methods.\n",
    "Supports batch processing of entire ShapeNet categories with multiple output formats.\n",
    "\n",
    "Usage:\n",
    "    python mesh_to_pointcloud.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ccd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "import argparse\n",
    "from typing import List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc790e6c",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "CONFIGURATION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a136540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input/Output directories\n",
    "SHAPENET_DIR = \"../src/data/shapenet_extracted\"        # Where your extracted ShapeNet data is\n",
    "POINTCLOUD_DIR = \"../src/data/pointclouds\"             # Where to save point clouds\n",
    "METADATA_DIR = \"../src/data/metadata\"                  # Where to save processing metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeaf2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point cloud generation settings\n",
    "DEFAULT_NUM_POINTS = 2048                    # Default number of points to sample\n",
    "POINT_CLOUD_FORMATS = ['npy', 'ply']         # Output formats ('npy', 'ply', 'txt')\n",
    "SAMPLING_METHODS = ['surface', 'volume']     # surface = mesh surface, volume = solid interior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a505bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing settings\n",
    "MAX_WORKERS = multiprocessing.cpu_count()   # Number of parallel processes\n",
    "BATCH_SIZE = 100                            # Process files in batches\n",
    "SKIP_EXISTING = True                        # Skip already processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b0d68",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Quality settings\n",
    "MIN_VERTICES = 10                           # Skip meshes with too few vertices\n",
    "MAX_VERTICES = 1000000                      # Skip meshes that are too large\n",
    "REMOVE_DUPLICATES = True                    # Remove duplicate vertices\n",
    "FIX_NORMALS = True                         # Attempt to fix mesh normals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189f63c",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "UTILITY FUNCTIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2483ac7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    \"\"\"Create necessary output directories\"\"\"\n",
    "    dirs = [POINTCLOUD_DIR, METADATA_DIR]\n",
    "    for dir_path in dirs:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        print(f\"üìÅ Created directory: {dir_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddcc634",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def find_obj_files(shapenet_dir):\n",
    "    \"\"\"Find all OBJ files in the ShapeNet directory structure\"\"\"\n",
    "    print(f\"üîç Scanning for OBJ files in {shapenet_dir}...\")\n",
    "    \n",
    "    obj_files = []\n",
    "    shapenet_path = Path(shapenet_dir)\n",
    "    \n",
    "    if not shapenet_path.exists():\n",
    "        print(f\"‚ùå ShapeNet directory not found: {shapenet_dir}\")\n",
    "        return []\n",
    "    \n",
    "    # Find all .obj files recursively\n",
    "    for obj_file in shapenet_path.rglob(\"*.obj\"):\n",
    "        # Extract category and model ID from path\n",
    "        parts = obj_file.parts\n",
    "        \n",
    "        # Typical ShapeNet structure: shapenet_extracted/category/model_id/models/model_normalized.obj\n",
    "        if len(parts) >= 3:\n",
    "            category = None\n",
    "            model_id = None\n",
    "            \n",
    "            # Find category (8-digit number)\n",
    "            for part in parts:\n",
    "                if len(part) == 8 and part.isdigit():\n",
    "                    category = part\n",
    "                    break\n",
    "            \n",
    "            # Find model ID (32-character hash)\n",
    "            for part in parts:\n",
    "                if len(part) == 32:\n",
    "                    model_id = part\n",
    "                    break\n",
    "            \n",
    "            obj_files.append({\n",
    "                'file_path': str(obj_file),\n",
    "                'category': category,\n",
    "                'model_id': model_id,\n",
    "                'filename': obj_file.name,\n",
    "                'size_mb': obj_file.stat().st_size / (1024 * 1024) if obj_file.exists() else 0\n",
    "            })\n",
    "    \n",
    "    print(f\"‚úì Found {len(obj_files)} OBJ files\")\n",
    "    \n",
    "    # Group by category\n",
    "    categories = {}\n",
    "    for obj in obj_files:\n",
    "        cat = obj['category']\n",
    "        if cat:\n",
    "            if cat not in categories:\n",
    "                categories[cat] = []\n",
    "            categories[cat].append(obj)\n",
    "    \n",
    "    print(f\"üìä Categories found:\")\n",
    "    for cat, objs in categories.items():\n",
    "        print(f\"  {cat}: {len(objs)} models\")\n",
    "    \n",
    "    return obj_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c9ea7a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_mesh(file_path):\n",
    "    \"\"\"Load a mesh file using trimesh\"\"\"\n",
    "    try:\n",
    "        mesh = trimesh.load(file_path, force='mesh')\n",
    "        \n",
    "        # Handle multiple meshes (trimesh sometimes returns a Scene)\n",
    "        if hasattr(mesh, 'geometry'):\n",
    "            # It's a Scene, get the first mesh\n",
    "            if len(mesh.geometry) > 0:\n",
    "                mesh = list(mesh.geometry.values())[0]\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        return mesh\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd44e93",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "POINT CLOUD GENERATION FUNCTIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29352cc8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def mesh_to_pointcloud_surface(mesh, num_points=DEFAULT_NUM_POINTS):\n",
    "    \"\"\"Sample points from mesh surface\"\"\"\n",
    "    try:\n",
    "        # Sample points from mesh surface\n",
    "        points, face_indices = mesh.sample(num_points, return_index=True)\n",
    "        \n",
    "        # Get face normals for the sampled points\n",
    "        if mesh.face_normals is not None and len(mesh.face_normals) > 0:\n",
    "            normals = mesh.face_normals[face_indices]\n",
    "        else:\n",
    "            # Compute normals if not available\n",
    "            try:\n",
    "                mesh.compute_vertex_normals()\n",
    "                normals = mesh.face_normals[face_indices] if mesh.face_normals is not None else None\n",
    "            except:\n",
    "                normals = None\n",
    "        \n",
    "        return points, normals\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error sampling surface: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b005f1ac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def mesh_to_pointcloud_volume(mesh, num_points=DEFAULT_NUM_POINTS):\n",
    "    \"\"\"Sample points from mesh volume (solid interior)\"\"\"\n",
    "    try:\n",
    "        # Check if mesh is watertight\n",
    "        if not mesh.is_watertight:\n",
    "            print(\"‚ö†Ô∏è  Mesh is not watertight, using surface sampling instead\")\n",
    "            return mesh_to_pointcloud_surface(mesh, num_points)\n",
    "        \n",
    "        # Sample points from volume\n",
    "        points = mesh.sample_volume(num_points)\n",
    "        \n",
    "        # For volume sampling, normals are not well-defined\n",
    "        normals = None\n",
    "        \n",
    "        return points, normals\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error sampling volume: {e}\")\n",
    "        # Fallback to surface sampling\n",
    "        return mesh_to_pointcloud_surface(mesh, num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b08c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_mesh(mesh):\n",
    "    \"\"\"Preprocess mesh before point cloud generation\"\"\"\n",
    "    if mesh is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Remove duplicate vertices\n",
    "        if REMOVE_DUPLICATES:\n",
    "            mesh.remove_duplicate_faces()\n",
    "            mesh.remove_unreferenced_vertices()\n",
    "        \n",
    "        # Fix normals\n",
    "        if FIX_NORMALS:\n",
    "            try:\n",
    "                mesh.fix_normals()\n",
    "            except:\n",
    "                pass  # Some meshes can't have normals fixed\n",
    "        \n",
    "        # Check mesh quality\n",
    "        if len(mesh.vertices) < MIN_VERTICES:\n",
    "            print(f\"‚ùå Mesh has too few vertices: {len(mesh.vertices)}\")\n",
    "            return None\n",
    "        \n",
    "        if len(mesh.vertices) > MAX_VERTICES:\n",
    "            print(f\"‚ùå Mesh has too many vertices: {len(mesh.vertices)}\")\n",
    "            return None\n",
    "        \n",
    "        return mesh\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error preprocessing mesh: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f19032",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "FILE I/O FUNCTIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc68242",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_pointcloud_npy(points, normals, output_path):\n",
    "    \"\"\"Save point cloud as NumPy array\"\"\"\n",
    "    try:\n",
    "        if normals is not None:\n",
    "            # Combine points and normals\n",
    "            data = np.hstack([points, normals])\n",
    "        else:\n",
    "            data = points\n",
    "        \n",
    "        np.save(output_path, data)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving NPY: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3257d2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_pointcloud_ply(points, normals, output_path):\n",
    "    \"\"\"Save point cloud as PLY file\"\"\"\n",
    "    try:\n",
    "        # Create a point cloud using trimesh\n",
    "        if normals is not None:\n",
    "            pointcloud = trimesh.PointCloud(vertices=points, vertex_normals=normals)\n",
    "        else:\n",
    "            pointcloud = trimesh.PointCloud(vertices=points)\n",
    "        \n",
    "        pointcloud.export(output_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving PLY: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d4f4d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_pointcloud_txt(points, normals, output_path):\n",
    "    \"\"\"Save point cloud as text file\"\"\"\n",
    "    try:\n",
    "        if normals is not None:\n",
    "            # Format: x y z nx ny nz\n",
    "            data = np.hstack([points, normals])\n",
    "            header = \"x y z nx ny nz\"\n",
    "        else:\n",
    "            # Format: x y z\n",
    "            data = points\n",
    "            header = \"x y z\"\n",
    "        \n",
    "        np.savetxt(output_path, data, header=header, comments='# ')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving TXT: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad497eed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_pointcloud(points, normals, base_path, formats):\n",
    "    \"\"\"Save point cloud in multiple formats\"\"\"\n",
    "    saved_files = []\n",
    "    \n",
    "    for fmt in formats:\n",
    "        output_path = f\"{base_path}.{fmt}\"\n",
    "        \n",
    "        if fmt == 'npy':\n",
    "            success = save_pointcloud_npy(points, normals, output_path)\n",
    "        elif fmt == 'ply':\n",
    "            success = save_pointcloud_ply(points, normals, output_path)\n",
    "        elif fmt == 'txt':\n",
    "            success = save_pointcloud_txt(points, normals, output_path)\n",
    "        else:\n",
    "            print(f\"‚ùå Unknown format: {fmt}\")\n",
    "            continue\n",
    "        \n",
    "        if success:\n",
    "            saved_files.append(output_path)\n",
    "    \n",
    "    return saved_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b05ef1",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "PROCESSING FUNCTIONS\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa219b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_single_mesh(args):\n",
    "    \"\"\"Process a single mesh file (for parallel processing)\"\"\"\n",
    "    obj_info, num_points, sampling_methods, output_formats = args\n",
    "    \n",
    "    file_path = obj_info['file_path']\n",
    "    category = obj_info['category']\n",
    "    model_id = obj_info['model_id']\n",
    "    \n",
    "    print(f\"üîÑ Processing: {category}/{model_id}\")\n",
    "    \n",
    "    results = {\n",
    "        'file_path': file_path,\n",
    "        'category': category,\n",
    "        'model_id': model_id,\n",
    "        'success': False,\n",
    "        'error': None,\n",
    "        'output_files': [],\n",
    "        'mesh_stats': {},\n",
    "        'processing_time': 0\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Load mesh\n",
    "        mesh = load_mesh(file_path)\n",
    "        if mesh is None:\n",
    "            results['error'] = \"Failed to load mesh\"\n",
    "            return results\n",
    "        \n",
    "        # Preprocess mesh\n",
    "        mesh = preprocess_mesh(mesh)\n",
    "        if mesh is None:\n",
    "            results['error'] = \"Mesh failed preprocessing\"\n",
    "            return results\n",
    "        \n",
    "        # Record mesh statistics\n",
    "        results['mesh_stats'] = {\n",
    "            'vertices': len(mesh.vertices),\n",
    "            'faces': len(mesh.faces),\n",
    "            'watertight': mesh.is_watertight,\n",
    "            'volume': float(mesh.volume) if mesh.is_watertight else None,\n",
    "            'surface_area': float(mesh.area),\n",
    "            'bounds': mesh.bounds.tolist()\n",
    "        }\n",
    "        \n",
    "        # Create output directory structure\n",
    "        category_dir = os.path.join(POINTCLOUD_DIR, category)\n",
    "        os.makedirs(category_dir, exist_ok=True)\n",
    "        \n",
    "        # Process each sampling method\n",
    "        for method in sampling_methods:\n",
    "            # Generate point cloud\n",
    "            if method == 'surface':\n",
    "                points, normals = mesh_to_pointcloud_surface(mesh, num_points)\n",
    "            elif method == 'volume':\n",
    "                points, normals = mesh_to_pointcloud_volume(mesh, num_points)\n",
    "            else:\n",
    "                print(f\"‚ùå Unknown sampling method: {method}\")\n",
    "                continue\n",
    "            \n",
    "            if points is None:\n",
    "                continue\n",
    "            \n",
    "            # Create output filename\n",
    "            base_name = f\"{model_id}_{method}_{num_points}\"\n",
    "            base_path = os.path.join(category_dir, base_name)\n",
    "            \n",
    "            # Skip if files already exist\n",
    "            if SKIP_EXISTING:\n",
    "                existing_files = [f\"{base_path}.{fmt}\" for fmt in output_formats]\n",
    "                if all(os.path.exists(f) for f in existing_files):\n",
    "                    print(f\"‚è≠Ô∏è  Skipping {base_name} (already exists)\")\n",
    "                    results['output_files'].extend(existing_files)\n",
    "                    continue\n",
    "            \n",
    "            # Save point cloud\n",
    "            saved_files = save_pointcloud(points, normals, base_path, output_formats)\n",
    "            results['output_files'].extend(saved_files)\n",
    "        \n",
    "        results['success'] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['error'] = str(e)\n",
    "        print(f\"‚ùå Error processing {file_path}: {e}\")\n",
    "    \n",
    "    results['processing_time'] = time.time() - start_time\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714f2cc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_batch(obj_files, num_points=DEFAULT_NUM_POINTS, \n",
    "                 sampling_methods=SAMPLING_METHODS, \n",
    "                 output_formats=POINT_CLOUD_FORMATS,\n",
    "                 max_workers=MAX_WORKERS):\n",
    "    \"\"\"Process a batch of mesh files\"\"\"\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting batch processing of {len(obj_files)} files\")\n",
    "    print(f\"   Workers: {max_workers}\")\n",
    "    print(f\"   Points per cloud: {num_points}\")\n",
    "    print(f\"   Sampling methods: {sampling_methods}\")\n",
    "    print(f\"   Output formats: {output_formats}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Prepare arguments for parallel processing\n",
    "    args_list = [\n",
    "        (obj_info, num_points, sampling_methods, output_formats)\n",
    "        for obj_info in obj_files\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # Process in parallel\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_mesh, args) for args in args_list]\n",
    "        \n",
    "        for i, future in enumerate(as_completed(futures)):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                \n",
    "                if result['success']:\n",
    "                    successful += 1\n",
    "                    print(f\"‚úì {result['category']}/{result['model_id']} \"\n",
    "                          f\"({result['processing_time']:.1f}s)\")\n",
    "                else:\n",
    "                    failed += 1\n",
    "                    print(f\"‚ùå {result['category']}/{result['model_id']}: {result['error']}\")\n",
    "                \n",
    "                # Progress update\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f\"üìä Progress: {i + 1}/{len(obj_files)} \"\n",
    "                          f\"(‚úì{successful} ‚ùå{failed})\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                failed += 1\n",
    "                print(f\"‚ùå Processing error: {e}\")\n",
    "    \n",
    "    return results, successful, failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae74a3",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "ANALYSIS AND REPORTING\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf8afd1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Analyze processing results and generate statistics\"\"\"\n",
    "    print(f\"\\nüìä PROCESSING ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    successful_results = [r for r in results if r['success']]\n",
    "    failed_results = [r for r in results if not r['success']]\n",
    "    \n",
    "    print(f\"Total files processed: {len(results)}\")\n",
    "    print(f\"Successful: {len(successful_results)}\")\n",
    "    print(f\"Failed: {len(failed_results)}\")\n",
    "    print(f\"Success rate: {len(successful_results)/len(results)*100:.1f}%\")\n",
    "    \n",
    "    if successful_results:\n",
    "        # Processing time statistics\n",
    "        times = [r['processing_time'] for r in successful_results]\n",
    "        print(f\"\\nProcessing time statistics:\")\n",
    "        print(f\"  Average: {np.mean(times):.2f}s\")\n",
    "        print(f\"  Median: {np.median(times):.2f}s\")\n",
    "        print(f\"  Min: {np.min(times):.2f}s\")\n",
    "        print(f\"  Max: {np.max(times):.2f}s\")\n",
    "        \n",
    "        # Mesh statistics\n",
    "        vertex_counts = [r['mesh_stats']['vertices'] for r in successful_results if 'vertices' in r['mesh_stats']]\n",
    "        if vertex_counts:\n",
    "            print(f\"\\nMesh complexity statistics:\")\n",
    "            print(f\"  Average vertices: {np.mean(vertex_counts):.0f}\")\n",
    "            print(f\"  Median vertices: {np.median(vertex_counts):.0f}\")\n",
    "            print(f\"  Min vertices: {np.min(vertex_counts):.0f}\")\n",
    "            print(f\"  Max vertices: {np.max(vertex_counts):.0f}\")\n",
    "        \n",
    "        # Category breakdown\n",
    "        categories = {}\n",
    "        for result in successful_results:\n",
    "            cat = result['category']\n",
    "            if cat:\n",
    "                categories[cat] = categories.get(cat, 0) + 1\n",
    "        \n",
    "        print(f\"\\nCategory breakdown:\")\n",
    "        for cat, count in sorted(categories.items()):\n",
    "            print(f\"  {cat}: {count} models\")\n",
    "    \n",
    "    # Error analysis\n",
    "    if failed_results:\n",
    "        print(f\"\\nError analysis:\")\n",
    "        error_types = {}\n",
    "        for result in failed_results:\n",
    "            error = result['error'] or 'Unknown error'\n",
    "            error_types[error] = error_types.get(error, 0) + 1\n",
    "        \n",
    "        for error, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {error}: {count} occurrences\")\n",
    "    \n",
    "    return {\n",
    "        'total': len(results),\n",
    "        'successful': len(successful_results),\n",
    "        'failed': len(failed_results),\n",
    "        'success_rate': len(successful_results)/len(results) if results else 0,\n",
    "        'categories': categories if successful_results else {},\n",
    "        'error_types': error_types if failed_results else {}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e778b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_processing_report(results, analysis, output_path):\n",
    "    \"\"\"Save detailed processing report\"\"\"\n",
    "    report = {\n",
    "        'summary': analysis,\n",
    "        'processing_details': results,\n",
    "        'configuration': {\n",
    "            'num_points': DEFAULT_NUM_POINTS,\n",
    "            'sampling_methods': SAMPLING_METHODS,\n",
    "            'output_formats': POINT_CLOUD_FORMATS,\n",
    "            'max_workers': MAX_WORKERS\n",
    "        },\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(report, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üìÑ Processing report saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79f2743",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "MAIN EXECUTION\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511b9af",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üéØ SHAPENET MESH TO POINT CLOUD CONVERTER\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Setup\n",
    "    setup_directories()\n",
    "    \n",
    "    # Find OBJ files\n",
    "    obj_files = find_obj_files(SHAPENET_DIR)\n",
    "    if not obj_files:\n",
    "        print(\"‚ùå No OBJ files found!\")\n",
    "        return\n",
    "    \n",
    "    # User options\n",
    "    print(f\"\\n‚öôÔ∏è  Configuration:\")\n",
    "    print(f\"   Input directory: {SHAPENET_DIR}\")\n",
    "    print(f\"   Output directory: {POINTCLOUD_DIR}\")\n",
    "    print(f\"   Found models: {len(obj_files)}\")\n",
    "    print(f\"   Points per cloud: {DEFAULT_NUM_POINTS}\")\n",
    "    print(f\"   Sampling methods: {SAMPLING_METHODS}\")\n",
    "    print(f\"   Output formats: {POINT_CLOUD_FORMATS}\")\n",
    "    print(f\"   Max workers: {MAX_WORKERS}\")\n",
    "    \n",
    "    # Confirm processing\n",
    "    proceed = input(f\"\\n‚ùì Process {len(obj_files)} models? (y/n): \").strip().lower()\n",
    "    if proceed != 'y':\n",
    "        print(\"‚ùå Processing cancelled\")\n",
    "        return\n",
    "    \n",
    "    # Process files\n",
    "    start_time = time.time()\n",
    "    results, successful, failed = process_batch(obj_files)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Analyze results\n",
    "    analysis = analyze_results(results)\n",
    "    \n",
    "    # Save report\n",
    "    report_path = os.path.join(METADATA_DIR, f\"processing_report_{int(time.time())}.json\")\n",
    "    save_processing_report(results, analysis, report_path)\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nüéâ PROCESSING COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚è±Ô∏è  Total time: {total_time/60:.1f} minutes\")\n",
    "    print(f\"üìä Processed: {successful} successful, {failed} failed\")\n",
    "    print(f\"üìÅ Output directory: {POINTCLOUD_DIR}\")\n",
    "    print(f\"üìÑ Report: {report_path}\")\n",
    "    \n",
    "    if successful > 0:\n",
    "        total_files = successful * len(SAMPLING_METHODS) * len(POINT_CLOUD_FORMATS)\n",
    "        print(f\"üéØ Generated ~{total_files} point cloud files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
